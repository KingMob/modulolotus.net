[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.5.2","content-config-digest","4c5ac3229df1e1ef","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://modulolotus.net\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{\"/posts/[...slug]\":\"/blog/[...slug]\"},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"svg\":{\"mode\":\"inline\"},\"serializeConfig\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,25,26,80,81,114,115,170,171,215,216,238,239,302,303,341,342,399,400],"2018-05-31-clojure-trie-performance",{"id":11,"data":13,"body":21,"filePath":22,"digest":23,"deferredRender":24},{"title":14,"description":15,"pubDate":16,"tags":17},"A case study in refactoring Clojure trie code for performance ","A detailed exploration of optimizing Clojure trie implementation for performance, showing the progression from idiomatic but slow code to highly optimized JVM-specific implementations with significant speed improvements through algorithmic changes and JVM optimizations.",["Date","2018-05-31T00:00:00.000Z"],[18,19,20],"Clojure","performance","trie","\u003Clink rel=\"stylesheet\" href=\"/supplemental/chartist.css\">\u003C/link>\n\u003Clink rel=\"stylesheet\" href=\"/supplemental/chart.css\">\u003C/link>\n    \nLast summer, I was doing HackerRank for fun and whiteboard practice, and I came across [a nifty little trie exercise](https://www.hackerrank.com/challenges/ctci-contacts/problem). The challenge was to add a list of contacts to a trie, and report on the number of contacts beginning with a list of prefixes.\n\nI used my favorite language at the time, Clojure, and quickly arrived at the correct solution, but many of these coding exercise sites have time constraints, and the idiomatic Clojure was too slow.\n\nWhat follows is how you take beautiful Clojure, and accelerate it when needed. (NB: Only do this for hot paths, this is not general Clojure style advice.) All the code is available [here](https://github.com/KingMob/clojure-trie-performance).\n\n## A Clojure Performance Journey\n\nFor those who don't recall, tries are specialized data structures that excel at storing data with common prefixes (e.g., words). Conceptually, it's a tree, where each node represents part of the prefix, and the complete path to a terminal node represents the data.\n\nFor a list of English words, the most straightforward implementation is a tree of nodes, where each node (other than the top) has a letter, a `terminal` flag to indicate whether the node is the last letter in a word, and an array of 26 pointers to other nodes, representing the alphabet. (Various optimizations exist to compress long chains, but we will focus on this implementation for now.)\n\nIn this diagram, you can see a representation of a trie storing the words: a, ale, all, alley, are, art, at, and ate. (Terminal nodes are tinted.)\n\n\u003Cdiv class=\"bg-white mx-auto py-1 rounded-md\">\n  \u003Cimg src=\"/supplemental/Clojure-trie-example.svg\" alt=\"trie example\" />\n\u003C/div>\n\n## Solutions\n\n#### Standard data structures\n\nHere's the basic implementation. It has functions that add new words, locate the partial subtree with a given prefix, and count the number of words beginning with a prefix. In this example, `db` is a series of nested hash-maps, and the `:*` key indicates the node is terminal.\n\n```clojure\n(defn add [db name]\n  (update-in db (seq name) (fnil assoc {}) :* true))\n\n(defn count-terminations [db]\n  (let [terminations (if (:* db) 1 0)]\n    (reduce +\n            terminations\n            (map count-terminations\n                 (vals (dissoc db :*))))))\n\n(defn find-partial [db partial]\n  (println\n   (if-let [sub-db (get-in db (seq partial))]\n     (count-terminations sub-db)\n     0)))\n```\n\nThis works, but was way too slow.\n\n#### Switch to eager over lazy evaluation\n\nClojure defaults to lazy evaluation, which requires a certain amount of overhead. What if we force eager evaluation with `transduce` instead of `reduce`?\n\n```clojure\n(defn count-terminations [db]\n  (let [terminations (if (:* db) 1 0)]\n    (transduce\n     (map count-terminations)\n     +\n     terminations\n     (vals db))))\n```\nThat shaves off a few seconds, but still not good enough.\n\n#### Switch to a record\n\nAlright, well, what about using a record with named fields and cache the default empty node?\n\n```clojure\n(declare default-alphabet-trie-node)\n\n(def empty-alphabet-vector (vec (repeat 26 nil)))\n\n(defrecord AlphabetTrieNode [val terminates? children]\n  TrieNode\n  (add-substring [n [c & cs]]\n    (->AlphabetTrieNode\n     val\n     (if c terminates? true)\n     (if c\n       (update children\n               (alpha-idx c)\n               #(add-substring (if (nil? %)\n                                 (default-alphabet-trie-node c)\n                                 %)\n                               cs))\n       children)))\n\n  (prefix [n s]\n    (->> s\n         (seq)\n         (map alpha-idx)\n         (interpose :children)\n         (cons :children)\n         (get-in n)))\n\n  (count-words [n]\n    (loop [word-count (if terminates? 1 0)\n           legit-children (filter some? children)]\n      (if (seq legit-children)\n        (let [[child & cs] legit-children]\n          (recur (+ word-count\n                    (if (:terminates? child) 1 0))\n                 (apply conj cs\n                        (filter some? (:children child)))))\n        word-count)))\n\n  (count-w-prefix [n s]\n    (if-let [subn (prefix n s)]\n      (count-words subn)\n      0)))\n\n(def default-alphabet-trie-node\n  (memoize\n   (fn [val]\n     (->AlphabetTrieNode val false empty-alphabet-vector))))\n```\n\nOof, no, the code is both slower and way more complicated. The performance benefit of records/types is that field access is much faster, which we'll exploit later.\n\n## Algorithmic/data change\n\nOK, let's re-evaluate, profile, and rethink the problem. (Tweaking rarely beats using the right data structures/algorithms.) We can trade off a bit of memory to save a ton of computation time. Instead of recomputing the subtree count afresh each time, we can keep track of the word count at each node, and increase as we go. Every time we add a word, we just increment the count of each parent node by 1. Then, the `count` operation for a prefix is just a read-out of the value at that node. \n\nJust to check, I applied this to the original solution, and got a speed-up of 10x, but it still wasn't fast enough, and using records enables some unique JVM optimizations, so we'll continue with that. Here are the changed parts:\n\n```clojure\n(defrecord AlphabetTrieNode [val terminates? word-count children]\n  TrieNode\n  (add-substring [n [c & cs]]\n    (->AlphabetTrieNode\n     val\n     (if c terminates? true)\n     (inc word-count)\n     (if c\n       (update children\n               (alpha-idx c)\n               (fnil #(add-substring % cs)\n                     (default-alphabet-trie-node c)))\n       children)))\n\n  (count-words [n]\n    word-count)\n\n  (count-w-prefix [n s]\n    (if-let [subn (prefix n s)]\n      (count-words subn)\n      0)))\n```\n\nNow this speeds up by a factor of 50, and is simpler to boot! We're getting closer. Takeaway: always, always use the right data structures/algorithms.\n\n## JVM optimizations\n\nClojure uses immutable data by default, for simplicity, ease of reasoning, and thread safety. But immutable data structures have an inherent overhead when \"mutating\": copies are unavoidable. What if we ditch immutability? \n\n#### Mutable fields\n\nWe can do this by adding metadata to fields indicating they're `volatile-mutable`. They can then be directly mutated in code.\n\n```clojure\n(deftype AlphabetTrieNode [val\n                           ^:volatile-mutable terminates?\n                           ^:volatile-mutable word-count\n                           ^:volatile-mutable children]\n  TrieNode\n  (add-substring [n [c & cs]]\n    (set! word-count (inc word-count))\n    (if-not c\n      (set! terminates? true)\n      (let [i (alpha-idx c)\n            child (children i)]\n        (when-not child\n          (->> c\n               (default-alphabet-trie-node)\n               (assoc children i)\n               (set! children)))\n        (add-substring (children i) cs))))\n\n  (prefix [n s]\n    (loop [curr n\n           [c & cs] s]\n      (if (and c curr)\n        (recur (get (.children curr) (alpha-idx c))\n               cs)\n        curr)))\n```\n\nNote the use of `set!` in the mutable code. We're finally seeing subsecond execution time for this exercise.\n\n#### Thread-unsafe with type hints\n\nWhat else can we do? Well, if we don't care about thread safety, we can switch to `unsynchronized-mutable` fields to avoid concurrency overhead. We can also switch to Java primitives and arrays with type hints. (The `val` field was also removed, since it's redundant to the `children` index.)\n\n```clojure\n(deftype AlphabetTrieNode [^:unsynchronized-mutable terminates?\n                           ^:unsynchronized-mutable ^long word-count\n                           ^:unsychronized-mutable ^objects children]\n  TrieNode\n  (add-substring [n [c & cs]]\n    (set! word-count (inc word-count))\n    (if-not c\n      (set! terminates? true)\n      (let [i (int (alpha-idx c))\n            child (aget children i)]\n        (when-not child\n          (aset children i (default-alphabet-trie-node c)))\n        (add-substring (aget children i) cs))))\n```\n\nGreat, we're down to a half second now, and fast enough for HackerRank's picky tests. Done!\n\n## Alternatives\n\nThere are other performance-enhancing techniques that either didn't apply here or didn't have an effect on the speed in this particular case (e.g., reflection was never an issue here), or were just paths not taken.\n\n#### Transients\n\nTransients are a way to use mutable data structures with code that has the same shape as your regular immutable code. Unfortunately, they do not work with records/types. They helped a bit with the hash-maps, but only by ~20%.\n\n#### Reflection and type-hints\n\nIf the compiler can't figure out what a data type is when invoking a method, it will slow things down massively. Use `(set! *warn-on-reflection* true)` in a file to check. I tested this, but there was no reflection in the hot path.\n\nUnfortunately, it's not really possible to type-hint protocol method parameters, and you can't defer to a regular helper function with mutable fields, since mutable fields are private. At that point, you may want to try another method or use `definterface`.\n\n#### Loop/recur\n\nIf a `loop` or function returns a value in the tail position, the current stack frame can be safely overwritten with the new value. `recur` can be used to avoid blowing up a deep stack, and it probably eased memory pressure here, but I didn't analyze its performance effect separately.\n\n## Results\nHere are the raw results. The first four are for standard hash-maps, the remainder use records/types.\n\n\u003Cdiv class=\"bg-white mx-auto p-2 rounded-md\">\n  \u003Cdiv class=\"ct-chart ct-perfect-fourth\">\u003C/div>\n\u003C/div>\n\n\nHope you found this useful. Thanks to David Nolen for the feedback!\n\n\u003Cscript src=\"/supplemental/chartist.min.js\">\u003C/script>\n\u003Cscript src=\"/supplemental/clojure-trie-performance-chart.js\">\u003C/script>","src/content/blog/2018-05-31-clojure-trie-performance.mdx","b356fdd476e18b42",true,"2017-05-17-thoughts-on-cryptopals-1",{"id":25,"data":27,"body":35,"filePath":36,"digest":37,"rendered":38},{"title":28,"description":29,"pubDate":30,"tags":31},"Using Clojure for the Cryptopals cryptography challenges","Insights and challenges from solving the Cryptopals cryptography exercises using Clojure, highlighting specific implementation issues, JVM limitations for byte manipulation, and offering tips for specific challenges.",["Date","2017-05-17T00:00:00.000Z"],[18,32,33,34],"cryptopals","Matasano","cryptography","I've been going through the [Cryptopals challenges](https://cryptopals.com/) recently. For those who don't know, they're a series of exercises based on real-life cryptography breaks, and they're lots of fun. You can see my code [here](https://github.com/KingMob/cryptopals-crypto-challenges).\n\nClojure is one of my favorite languages, so I decided to tackle them with it. Here are some thoughts on the first 3 sets, plus a few issues I ran into along the way. I hope this helps anyone else going through them without giving too much away.\n\n\n## General challenge notes\n### Challenge 3 - Single-byte XOR cipher\nWhen doing frequency analysis, the instructions make no mention of the chi-squared statistic, though that's probably a good starting point, and generalizes beyond alphabetic frequencies to any expected distribution. You can use it later when detecting ECB vs CBC, since you can use it to detect deviations from uniformity.\n\n### Challenge 7 - AES in ECB mode\nThe instructions *do* mention this, but after chatting with someone else doing the challenges, realize that you do *not* need to write AES yourself. Just use a default implementation for your language. You only need the single block transform (ECB), which you'll use to build on top of. For Java, all you need is javax.crypto.Cipher and SecretKeySpec, using \"AES/ECB/NoPadding\".\n\n### Challenge 9 - Implement PKCS#7 padding\nWhen implementing PKCS#7 padding, the instructions make no mention of what to do when your data lines up perfectly on a block boundary. Only later, when doing the padding oracle challenge, did I realize you have to add an entire padding block (sixteen 16's) in that case.\n\n### Challenge 17 - The CBC padding oracle\nWhen altering the first byte of a block to see if it produces valid padding, you're aiming to alter it such that the padding ends in a single 1 byte. But *remember*, if you test the *original, unaltered* byte, you will *also* get valid PKCS#7 padding (since it was correct to begin with), which is probably not 1.\n\n### Challenge 18 - Implement CTR, the stream cipher mode\nLook carefully at the examples. At first glance it looks as if you're XORing a bunch of zeros, but the ninth byte is actually incrementing.\n\n### Challenge 23 - Clone an MT19937 RNG from its output\nInverting the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister)'s temper function has a computationally simple version. It's not necessary to use brute force, inverse matrix multiplication, or a constraint solver. It can be done with just bit operations. The key insight is that XORing an integer with a shifted version of itself results in many of the bits in the output being identical (since the shifted-in bits are 0). You can then shift *those* bits and XOR again to obtain *more* bits, repeating until you have the whole thing.\n\n## Clojure / Java-specific issues\n### Poor primitive support in JVM and Clojure\n\nThe bread and butter of cryptography is byte-level manipulation. Unfortunately, the JVM treats bytes as second-class citizens (e.g., many methods on Integer/Long have no Byte counterpart), and lacks support for unsigned integer types. On top of this, Clojure defaults to 64-bit longs everywhere, necessitating a lot of conversion. I ended up writing many conversion routines just for basic support, and leaving integers in the default long type for compatibility.\n\nThe single biggest thing to watch for was when I got deep into bit-shifting code while building the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister) random number generator. When everything is a long, unsigned right bit shifts **will not do what you expect**. The 32-bit version of the Mersenne Twister (MT) relies on unsigned 32-bit ints. But unsigned shifting adds zeros at the end, which, if you're using a larger data type, will result in the zeros entering *above* the bits of interest. E.g., consider the case where all bits are set to 1 (in two's complement, this is -1), and you do an unsigned shift right by 16 bits. You *want*:\n\n`00000000000000001111111111111111`\n\nbut you *get*:\n\n`0000000000000000111111111111111111111111111111111111111111111111`\n\nSince you only want the right-most 32 bits, it appears as if shifting had no effect!\n\n\n## Final Thoughts\n\n1. Some of these ciphers are so broken, I'm amazed they were ever used. Apparently, the simple substitution cipher (replace each letter with the letter *n* higher) was in use as late as 1915 by the Russian army, despite frequency analysis having been described a millennia (!) earlier.\n\n2. Almost everything decrypts to Vanilla Ice lyrics. Unfortunately, there's probably nothing hidden in *them*.","src/content/blog/2017-05-17-thoughts-on-cryptopals-1.md","1dcda45cc19fd425",{"html":39,"metadata":40},"\u003Cp>I’ve been going through the \u003Ca href=\"https://cryptopals.com/\">Cryptopals challenges\u003C/a> recently. For those who don’t know, they’re a series of exercises based on real-life cryptography breaks, and they’re lots of fun. You can see my code \u003Ca href=\"https://github.com/KingMob/cryptopals-crypto-challenges\">here\u003C/a>.\u003C/p>\n\u003Cp>Clojure is one of my favorite languages, so I decided to tackle them with it. Here are some thoughts on the first 3 sets, plus a few issues I ran into along the way. I hope this helps anyone else going through them without giving too much away.\u003C/p>\n\u003Ch2 id=\"general-challenge-notes\">General challenge notes\u003C/h2>\n\u003Ch3 id=\"challenge-3---single-byte-xor-cipher\">Challenge 3 - Single-byte XOR cipher\u003C/h3>\n\u003Cp>When doing frequency analysis, the instructions make no mention of the chi-squared statistic, though that’s probably a good starting point, and generalizes beyond alphabetic frequencies to any expected distribution. You can use it later when detecting ECB vs CBC, since you can use it to detect deviations from uniformity.\u003C/p>\n\u003Ch3 id=\"challenge-7---aes-in-ecb-mode\">Challenge 7 - AES in ECB mode\u003C/h3>\n\u003Cp>The instructions \u003Cem>do\u003C/em> mention this, but after chatting with someone else doing the challenges, realize that you do \u003Cem>not\u003C/em> need to write AES yourself. Just use a default implementation for your language. You only need the single block transform (ECB), which you’ll use to build on top of. For Java, all you need is javax.crypto.Cipher and SecretKeySpec, using “AES/ECB/NoPadding”.\u003C/p>\n\u003Ch3 id=\"challenge-9---implement-pkcs7-padding\">Challenge 9 - Implement PKCS#7 padding\u003C/h3>\n\u003Cp>When implementing PKCS#7 padding, the instructions make no mention of what to do when your data lines up perfectly on a block boundary. Only later, when doing the padding oracle challenge, did I realize you have to add an entire padding block (sixteen 16’s) in that case.\u003C/p>\n\u003Ch3 id=\"challenge-17---the-cbc-padding-oracle\">Challenge 17 - The CBC padding oracle\u003C/h3>\n\u003Cp>When altering the first byte of a block to see if it produces valid padding, you’re aiming to alter it such that the padding ends in a single 1 byte. But \u003Cem>remember\u003C/em>, if you test the \u003Cem>original, unaltered\u003C/em> byte, you will \u003Cem>also\u003C/em> get valid PKCS#7 padding (since it was correct to begin with), which is probably not 1.\u003C/p>\n\u003Ch3 id=\"challenge-18---implement-ctr-the-stream-cipher-mode\">Challenge 18 - Implement CTR, the stream cipher mode\u003C/h3>\n\u003Cp>Look carefully at the examples. At first glance it looks as if you’re XORing a bunch of zeros, but the ninth byte is actually incrementing.\u003C/p>\n\u003Ch3 id=\"challenge-23---clone-an-mt19937-rng-from-its-output\">Challenge 23 - Clone an MT19937 RNG from its output\u003C/h3>\n\u003Cp>Inverting the \u003Ca href=\"https://en.wikipedia.org/wiki/Mersenne_Twister\">Mersenne Twister\u003C/a>’s temper function has a computationally simple version. It’s not necessary to use brute force, inverse matrix multiplication, or a constraint solver. It can be done with just bit operations. The key insight is that XORing an integer with a shifted version of itself results in many of the bits in the output being identical (since the shifted-in bits are 0). You can then shift \u003Cem>those\u003C/em> bits and XOR again to obtain \u003Cem>more\u003C/em> bits, repeating until you have the whole thing.\u003C/p>\n\u003Ch2 id=\"clojure--java-specific-issues\">Clojure / Java-specific issues\u003C/h2>\n\u003Ch3 id=\"poor-primitive-support-in-jvm-and-clojure\">Poor primitive support in JVM and Clojure\u003C/h3>\n\u003Cp>The bread and butter of cryptography is byte-level manipulation. Unfortunately, the JVM treats bytes as second-class citizens (e.g., many methods on Integer/Long have no Byte counterpart), and lacks support for unsigned integer types. On top of this, Clojure defaults to 64-bit longs everywhere, necessitating a lot of conversion. I ended up writing many conversion routines just for basic support, and leaving integers in the default long type for compatibility.\u003C/p>\n\u003Cp>The single biggest thing to watch for was when I got deep into bit-shifting code while building the \u003Ca href=\"https://en.wikipedia.org/wiki/Mersenne_Twister\">Mersenne Twister\u003C/a> random number generator. When everything is a long, unsigned right bit shifts \u003Cstrong>will not do what you expect\u003C/strong>. The 32-bit version of the Mersenne Twister (MT) relies on unsigned 32-bit ints. But unsigned shifting adds zeros at the end, which, if you’re using a larger data type, will result in the zeros entering \u003Cem>above\u003C/em> the bits of interest. E.g., consider the case where all bits are set to 1 (in two’s complement, this is -1), and you do an unsigned shift right by 16 bits. You \u003Cem>want\u003C/em>:\u003C/p>\n\u003Cp>\u003Ccode>00000000000000001111111111111111\u003C/code>\u003C/p>\n\u003Cp>but you \u003Cem>get\u003C/em>:\u003C/p>\n\u003Cp>\u003Ccode>0000000000000000111111111111111111111111111111111111111111111111\u003C/code>\u003C/p>\n\u003Cp>Since you only want the right-most 32 bits, it appears as if shifting had no effect!\u003C/p>\n\u003Ch2 id=\"final-thoughts\">Final Thoughts\u003C/h2>\n\u003Col>\n\u003Cli>\n\u003Cp>Some of these ciphers are so broken, I’m amazed they were ever used. Apparently, the simple substitution cipher (replace each letter with the letter \u003Cem>n\u003C/em> higher) was in use as late as 1915 by the Russian army, despite frequency analysis having been described a millennia (!) earlier.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Almost everything decrypts to Vanilla Ice lyrics. Unfortunately, there’s probably nothing hidden in \u003Cem>them\u003C/em>.\u003C/p>\n\u003C/li>\n\u003C/ol>",{"headings":41,"localImagePaths":74,"remoteImagePaths":75,"frontmatter":76,"imagePaths":79},[42,46,50,53,56,59,62,65,68,71],{"depth":43,"slug":44,"text":45},2,"general-challenge-notes","General challenge notes",{"depth":47,"slug":48,"text":49},3,"challenge-3---single-byte-xor-cipher","Challenge 3 - Single-byte XOR cipher",{"depth":47,"slug":51,"text":52},"challenge-7---aes-in-ecb-mode","Challenge 7 - AES in ECB mode",{"depth":47,"slug":54,"text":55},"challenge-9---implement-pkcs7-padding","Challenge 9 - Implement PKCS#7 padding",{"depth":47,"slug":57,"text":58},"challenge-17---the-cbc-padding-oracle","Challenge 17 - The CBC padding oracle",{"depth":47,"slug":60,"text":61},"challenge-18---implement-ctr-the-stream-cipher-mode","Challenge 18 - Implement CTR, the stream cipher mode",{"depth":47,"slug":63,"text":64},"challenge-23---clone-an-mt19937-rng-from-its-output","Challenge 23 - Clone an MT19937 RNG from its output",{"depth":43,"slug":66,"text":67},"clojure--java-specific-issues","Clojure / Java-specific issues",{"depth":47,"slug":69,"text":70},"poor-primitive-support-in-jvm-and-clojure","Poor primitive support in JVM and Clojure",{"depth":43,"slug":72,"text":73},"final-thoughts","Final Thoughts",[],[],{"title":28,"pubDate":77,"tags":78,"description":29},["Date","2017-05-17T00:00:00.000Z"],[18,32,33,34],[],"2014-07-11-premature-optimization",{"id":80,"data":82,"body":89,"filePath":90,"digest":91,"rendered":92},{"title":83,"description":84,"pubDate":85,"tags":86},"Premature optimization is the root of all hair loss","A cautionary tale about focusing on algorithmic optimization before profiling. I wasted time implementing fancy graph algorithms for a puzzle only to discover that Java startup time was the actual bottleneck.",["Date","2014-07-11T00:00:00.000Z"],[87,88],"optimization","graph theory","When faced with a need to speed up code, the general advice is to profile it to get an accurate picture of where time is being spent. Many of you have heard by now the old Knuth quote, \"Premature optimization is the root of all evil.\" While I usually remember this, what follows is a bit of a cautionary tale of what happens when you *think* you know where the slowdown is (or will be), instead of bothering to actually find out.\n\n## The puzzle\nA while back, I decided to tackle Spotify's online puzzles. You can see my solutions [here](https://github.com/KingMob/Spotify-puzzles). The way it worked was, you coded up your answer in either C, C++, Python or Java, mailed it off to an automatic judge, and would then get a response back telling you if it failed on some input or wasn't fast enough. Two of the problems were quite easy, but the last turned out to be tricky, *though not as tricky as I made it*.\n\nYou can see an archive of the puzzle page [here](/supplemental/Bilateral%20Projects%20Puzzlecool%20-%20Spotify.html). The basic problem is that there are a bunch of Spotify employees in either New York or Sweden, and every employee in one location is a member of one or more paired teams with an employee in the other location. There's an upcoming Spotify meeting in Barbados, and someone from every team needs to be there to represent the team. To keep travel costs down, Spotify wants to determine the smallest set of people it needs to have all teams represented.\n\nNow, some of you who took an algorithms class recently may already recognize the problem, but for the rest of us, the structure is that of a bipartite graph, a graph where every vertex (person) can be divided up into two groups (location), and every edge (team) is only *between* the groups, and never *within* one. The problem of determining the minimal set of vertices that is attached to every edge is called the minimum vertex cover problem, and for bipartite graphs, Konig's theorem demonstrated that it's equivalent to a finding a maximum matching, which is the largest set(s) of edges that don't share any vertices.\n\nClearly, such a problem with deep roots in graph theory requires proper selection of the appropriate algorithm, right?\n\n## The algorithms\nI originally started in Java with a naive algorithm, continually picking the person who was on the most remaining teams. It seemed to work on the two small examples listed on the problem page, so I submitted it and... no, it gave incorrect answers. Well, I didn't have access to Spotify's test data, so I started making my own. I quickly realized that my solution must have been *way* off when it returned a list of people that exceeded the total number of employees in one country. Without even knowing the details of my randomly-generated data set, I knew that was wrong, because I could have sent fewer people by just choosing everyone in New York or Sweden. HR would have hated my solution as much as the employees would have loved it.\n\nSince the naive solution wasn't working, I knew there had to be more to the problem, so, I started digging around and found out it was a bipartite maximum matching graph problem. After that, I scanned the literature to choose the \"right\" algorithm. The Hopcroft-Karp algorithm has the best time, with an O(E√V) worst-case time, where E is the number of edges, and V is the number of vertices. But I chose the relabel-to-front variant of the push-relabel max-flow algorithm (Cormen et al, Introduction to Algorithms, 3rd ed., pp. 748), since according to some empirical papers (Cherkassky 1998; Setubal 1996), it outperforms the Hopcroft-Karp algorithm in practice, despite having a O(V\u003Csup>3\u003C/sup>) running time. It's one of the more complicated algorithms to code, but I was convinced algorithmic performance would be an issue. I wrote a Java version, submitted it, and... success, it returned correct answers for all inputs but wait... the automatic judge said it's too slow.\n\nHmm, maybe I needed to use the bipush selection variant to improve push-relabel performance. Code code code. Nope, still too slow. Maybe I should have tried the Hopcroft-Karp variant after all? I started on that, but then I thought, well, the problem size is actually pretty small, and big-O performance is only guaranteed asymptotically, so maybe the running time is being dominated by incidental overhead. Instead of pausing to consider, I wrote a simpler breadth-first search (BFS) for augmenting paths.\n\nNope, still too slow.\n\n## The \"Eureka\" moment\nAt this point, I wanted to `kill -9` some server process in Sweden. **But instead, I did what I should have done from the start: profiled.** The vertex selection algorithm took only ~200 ms on the largest problem size. Push-relabel and BFS were not too far apart. The remaining setup and I/O only added a bit more to the execution time. What was going on?\n\nThen finally, a horrific realization started to dawn on me. The code judge wasn't examining the algorithmically-interesting parts of my code (how could it?), it was just timing the whole thing from start to finish. So, with a feeling of dread in my stomach, I clocked the whole thing with the maximum problem size from the command-line and it took 5 seconds, only ~400 ms of which was my code. **The entire time was dominated by Java start-up; algorithms were completely irrelevant for the specified problem size.** At this point, everyone in the coffee shop probably heard me curse.\n\nSo what did I do? Well, I considered teaching myself Python, which I thought would be fun, but I suspected it might suffer the same startup overhead. In the end, I rewrote the entire thing using the BFS algorithm in C++, which I hadn't used since college. I submitted it and finally... success!\n\n## Coda\nWhat did I (re)learn?\n\n**Profile first.** Without knowing where/what to optimize, you're flying blind and possibly wasting time. Data trumps intuition in this case.","src/content/blog/2014-07-11-premature-optimization.md","78579363433c588a",{"html":93,"metadata":94},"\u003Cp>When faced with a need to speed up code, the general advice is to profile it to get an accurate picture of where time is being spent. Many of you have heard by now the old Knuth quote, “Premature optimization is the root of all evil.” While I usually remember this, what follows is a bit of a cautionary tale of what happens when you \u003Cem>think\u003C/em> you know where the slowdown is (or will be), instead of bothering to actually find out.\u003C/p>\n\u003Ch2 id=\"the-puzzle\">The puzzle\u003C/h2>\n\u003Cp>A while back, I decided to tackle Spotify’s online puzzles. You can see my solutions \u003Ca href=\"https://github.com/KingMob/Spotify-puzzles\">here\u003C/a>. The way it worked was, you coded up your answer in either C, C++, Python or Java, mailed it off to an automatic judge, and would then get a response back telling you if it failed on some input or wasn’t fast enough. Two of the problems were quite easy, but the last turned out to be tricky, \u003Cem>though not as tricky as I made it\u003C/em>.\u003C/p>\n\u003Cp>You can see an archive of the puzzle page \u003Ca href=\"/supplemental/Bilateral%20Projects%20Puzzlecool%20-%20Spotify.html\">here\u003C/a>. The basic problem is that there are a bunch of Spotify employees in either New York or Sweden, and every employee in one location is a member of one or more paired teams with an employee in the other location. There’s an upcoming Spotify meeting in Barbados, and someone from every team needs to be there to represent the team. To keep travel costs down, Spotify wants to determine the smallest set of people it needs to have all teams represented.\u003C/p>\n\u003Cp>Now, some of you who took an algorithms class recently may already recognize the problem, but for the rest of us, the structure is that of a bipartite graph, a graph where every vertex (person) can be divided up into two groups (location), and every edge (team) is only \u003Cem>between\u003C/em> the groups, and never \u003Cem>within\u003C/em> one. The problem of determining the minimal set of vertices that is attached to every edge is called the minimum vertex cover problem, and for bipartite graphs, Konig’s theorem demonstrated that it’s equivalent to a finding a maximum matching, which is the largest set(s) of edges that don’t share any vertices.\u003C/p>\n\u003Cp>Clearly, such a problem with deep roots in graph theory requires proper selection of the appropriate algorithm, right?\u003C/p>\n\u003Ch2 id=\"the-algorithms\">The algorithms\u003C/h2>\n\u003Cp>I originally started in Java with a naive algorithm, continually picking the person who was on the most remaining teams. It seemed to work on the two small examples listed on the problem page, so I submitted it and… no, it gave incorrect answers. Well, I didn’t have access to Spotify’s test data, so I started making my own. I quickly realized that my solution must have been \u003Cem>way\u003C/em> off when it returned a list of people that exceeded the total number of employees in one country. Without even knowing the details of my randomly-generated data set, I knew that was wrong, because I could have sent fewer people by just choosing everyone in New York or Sweden. HR would have hated my solution as much as the employees would have loved it.\u003C/p>\n\u003Cp>Since the naive solution wasn’t working, I knew there had to be more to the problem, so, I started digging around and found out it was a bipartite maximum matching graph problem. After that, I scanned the literature to choose the “right” algorithm. The Hopcroft-Karp algorithm has the best time, with an O(E√V) worst-case time, where E is the number of edges, and V is the number of vertices. But I chose the relabel-to-front variant of the push-relabel max-flow algorithm (Cormen et al, Introduction to Algorithms, 3rd ed., pp. 748), since according to some empirical papers (Cherkassky 1998; Setubal 1996), it outperforms the Hopcroft-Karp algorithm in practice, despite having a O(V\u003Csup>3\u003C/sup>) running time. It’s one of the more complicated algorithms to code, but I was convinced algorithmic performance would be an issue. I wrote a Java version, submitted it, and… success, it returned correct answers for all inputs but wait… the automatic judge said it’s too slow.\u003C/p>\n\u003Cp>Hmm, maybe I needed to use the bipush selection variant to improve push-relabel performance. Code code code. Nope, still too slow. Maybe I should have tried the Hopcroft-Karp variant after all? I started on that, but then I thought, well, the problem size is actually pretty small, and big-O performance is only guaranteed asymptotically, so maybe the running time is being dominated by incidental overhead. Instead of pausing to consider, I wrote a simpler breadth-first search (BFS) for augmenting paths.\u003C/p>\n\u003Cp>Nope, still too slow.\u003C/p>\n\u003Ch2 id=\"the-eureka-moment\">The “Eureka” moment\u003C/h2>\n\u003Cp>At this point, I wanted to \u003Ccode>kill -9\u003C/code> some server process in Sweden. \u003Cstrong>But instead, I did what I should have done from the start: profiled.\u003C/strong> The vertex selection algorithm took only ~200 ms on the largest problem size. Push-relabel and BFS were not too far apart. The remaining setup and I/O only added a bit more to the execution time. What was going on?\u003C/p>\n\u003Cp>Then finally, a horrific realization started to dawn on me. The code judge wasn’t examining the algorithmically-interesting parts of my code (how could it?), it was just timing the whole thing from start to finish. So, with a feeling of dread in my stomach, I clocked the whole thing with the maximum problem size from the command-line and it took 5 seconds, only ~400 ms of which was my code. \u003Cstrong>The entire time was dominated by Java start-up; algorithms were completely irrelevant for the specified problem size.\u003C/strong> At this point, everyone in the coffee shop probably heard me curse.\u003C/p>\n\u003Cp>So what did I do? Well, I considered teaching myself Python, which I thought would be fun, but I suspected it might suffer the same startup overhead. In the end, I rewrote the entire thing using the BFS algorithm in C++, which I hadn’t used since college. I submitted it and finally… success!\u003C/p>\n\u003Ch2 id=\"coda\">Coda\u003C/h2>\n\u003Cp>What did I (re)learn?\u003C/p>\n\u003Cp>\u003Cstrong>Profile first.\u003C/strong> Without knowing where/what to optimize, you’re flying blind and possibly wasting time. Data trumps intuition in this case.\u003C/p>",{"headings":95,"localImagePaths":108,"remoteImagePaths":109,"frontmatter":110,"imagePaths":113},[96,99,102,105],{"depth":43,"slug":97,"text":98},"the-puzzle","The puzzle",{"depth":43,"slug":100,"text":101},"the-algorithms","The algorithms",{"depth":43,"slug":103,"text":104},"the-eureka-moment","The “Eureka” moment",{"depth":43,"slug":106,"text":107},"coda","Coda",[],[],{"title":83,"pubDate":111,"tags":112,"description":84},["Date","2014-07-11T00:00:00.000Z"],[87,88],[],"2019-05-03-my-fav-idm",{"id":114,"data":116,"body":127,"filePath":128,"digest":129,"rendered":130},{"title":117,"description":118,"pubDate":119,"tags":120},"Arrhythmic Beats - IDM","A curated list of influential IDM artists and essential albums from the 90s, featuring Aphex Twin, Autechre, Boards of Canada, and more.",["Date","2019-05-03T00:00:00.000Z"],[121,122,123,124,125,126],"music","IDM","Aphex Twin","Autechre","Boards of Canada","Plaid","I promised a friend at a coffee shop I'd write up a list of my thoughts on IDM, aka \"Intelligent Dance Music\". It only took a year, but here it is.\n\nThe term “Intelligent Dance Music” emerged in the early 90s to describe a lot of the [Warp](https://warp.net/) label’s artists. At the time, Warp was aiming for a post-club, at-home kind of music. The idea was that the music had too many beats to be ambient, but was usually too rhythmically complex to actually be danced to. (While many of the artists involved dislike the term, no better term ever stuck. For a hot moment in the late 90’s, it seemed like it would be “electronica”, but that was broader in scope, and didn’t last.)\n\nMany of the artists seem to follow a certain progression: over time they become ever more experimental, and the music becomes more skittery and abstract. I *personally* love when artists successfully walk the fine line between ambient and dance, hooks and experiments, so I tend to favor most artists’ early/middle work. I personally believe that music should have a visceral component, that becoming too experimental robs it of something in favor of a head-heavy approach, and my reviews reflect that. But that’s just me.\n\nWhat follows are my personal favorite IDM artists and albums from that era, with random notes and asides.\n\n## Aphex Twin\n\nRichard D. James is amazingly talented, but also a bit of a cheeky wanker. He once lived in an old bank vault and drove a tank around town.\n\n- [Richard D. James Album](https://open.spotify.com/album/43s2fKRQsOSB6rSrxtAXKK?si=3dqIsPVrQHSOncZYy2xgxQ) - this is the one that kicked it off for me. Growing up in southern Virginia, I’d never heard anything like it. This is still my favorite era of Aphex Twin.\n- [...I Care Because You Do](https://open.spotify.com/album/0VG7XLJ8gSynEQDVnpHNNU?si=gsi06xUsQSmzGhqivTO1EA) - the album that preceded Richard D James, and similar in style. (Also check out the unreleased Melodies From Mars if you like this period.)\n- [Selected Ambient Works 85-92](https://open.spotify.com/album/7aNclGRxTysfh6z0d8671k?si=wVQqkx0vQ0iPAKbN-5TVaQ) - his first album is very danceable, since much of it was written when he was a teenage DJ.\n- [Selected Ambient Works 2](https://open.spotify.com/album/17vHPMmoxN5B8cdhCDeMTe?si=Wpk-ldBiSweyRIcHb83vmQ) - the album that made him famous - More ambient in tone, but with many interesting experiments that go way beyond Eno’s original ambient concepts.\n\nAphex Twin had multiple aliases, including AFX and Polygon Window. His own label released a zillion Analog Bubblebath compilations.\n\n## Boards of Canada\n\nDespite the name, they’re a couple of Scottish dudes who made music for sitting around the campfire with their art collective. Apparently they were big fans of the Film Board of Canada.\n\n- [Music Has the Right to Children](https://open.spotify.com/album/1vWnB0hYmluskQuzxwo25a?si=p5_2Ea94QxCZAX8gzciSqQ) - their first album, and their best. They coaxed a warmth rarely heard from electronic instruments.\n- [Twoism](https://open.spotify.com/album/0LDCQOBp7NFjkD1LbuusCG?si=25vz5CJJRFGVfHvumzjHIg) - an early EP off the Skam label, quite good\n\n## Autechre\n\nWhy these guys weren’t bigger, I’ve never understood. Both Aphex and Boards of Canada achieved breakout success, but not Autechre. The rumor was their name was short for “Audio Technology Research”, but they claimed the name was formed starting with “au” and then bashing the keyboard, which makes more sense, given their song titles.\n\n- [Incunabula](https://open.spotify.com/album/4KROnLN6Didp0F97RXaW7a?si=ZleIiQ_kQh-j0K2Y9jMVQg) - their first album, and one of their best.\n- [Amber](https://open.spotify.com/album/7EfhvG3RwdhzXrFlkDVxg4?si=m2IXR3UTSU2wyRDxi7afEw) - their followup album, and a bit more gentle rhythmically.\n- [Tri Repetae](https://open.spotify.com/album/0ioIXXMV89w0qC39FpxYnL?si=f6iiopZ5TIe8GoEJ55W8Ow) - pretty good\n- [Anvil Vapre](https://open.spotify.com/album/6UcShvT8JIKOaHj39k6A8f?si=nF4oeKGfRqeiitGobA6r2Q) - a harsh, almost metal version of their sound\n- [Garbage](https://open.spotify.com/album/31nThm3LyQVvrndqxhvjWl?si=E3d0O8zAQIqBYFMGfsp1Tw) - lush, more ambient, and one of the most beautiful things they ever created. I used to put this on after raves, and I forever associate it with driving back from one party the next morning in the rain while my friends snoozed in the car.\n- [LP5](https://open.spotify.com/album/7zlbMdBS3J2YQRDuMMT9u4?si=RY43hCupQ5C4WGU2_8RNog) - a transitional record. Some tracks are fantastic, like [Rae](https://open.spotify.com/track/4tBC5l4AkqYfQ5CCx5IrKm?si=awCYIrUBTAS9kIMXRBqMug) and [Corc](https://open.spotify.com/track/770zYtbMuU5grFPUIzxmfQ?si=-IYACGkgRwKQP9QNSQbcrQ), but overall, this marks Autechre’s shift into more experimental realms\n\nAlso worth hunting down are shadowy Gescom releases (rumored to be short for Gestalt Communications), which Ae frequently contributed to.\n\nFun little fact: they released an album, *Anti*, in response to a law criminalizing parties in Britain with a \"succession of repetitive beats\" by removing all repetition. It came with a sticker attesting to its legality, but suggested you still have a lawyer present just in case.\n\n## Plaid / The Black Dog\n\nWhile two different bands, they were originally one. Two of the founders of The Black Dog split off to make Plaid. They were associated with Bjork, and Plaid’s remix of her song, [All Is Full of Love](https://open.spotify.com/track/2ctHJYdYUsVPlJzhtQe6Jw?si=BEScs4pQQU6UOQqIvCFfzQ), is well worth a listen.\n\n- [Not For Threes](https://open.spotify.com/album/5O4LYrdtTr4nMnkn2GwP74?si=FKFg5GzzTyO_M5Jse5hoAg) - first album, standouts are [Myopia](https://open.spotify.com/track/70jjCa99FYeC4xvUmZy3Vf?si=Va_b0t42RR6AhpjZSVfmUA), [Kortisin](https://open.spotify.com/track/1sbTGYNgcr9YUSJoEY0oli?si=FZAxxmo5RdqJH7Ed6-ePyw), [Prague Radio](https://open.spotify.com/track/7FRNVeofO8OL094wfOM9Le?si=u-qWxwXaTEKtCTkYo_e9GA), [Rakimou](https://open.spotify.com/track/2ETmKAPTdY9DEIcWNo5IiZ?si=CHHPpNi3TtCTbPsBWQsUow), and [Lilith](https://open.spotify.com/track/3nSfJkSb7xRtrtJMKohXGc?si=_3K1WdAhRbev6ONR67tn_A), which Bjork guest vocals for\n- [Rest-Proof Clockwork](https://open.spotify.com/album/4IkvCDzidC5XbElsN1SUsg?si=Qch2ygPKSnuCOxh8XFixoQ) - the follow-up, also pretty good\n- The Digging Remedy - I was fairly luke-warm to a lot of their 2000's output, but I recently listened to this and it was way better than I expected\n\n## Mouse on Mars\n\nA couple of German fellows who crafted weird mixes of ambient, dub, and squiggly tunes. More playful than most artists of the era.\n\nCheck out [Autoditacker](https://open.spotify.com/album/4Ev3UAEmQeMotvOS6w2CO1?si=Fsb4ucUzTSW2m9__OPYBUA) and [Niun Niggung](https://open.spotify.com/album/73ilzyuAD5B5fhPekLrYiz?si=QMlkOuGlTaOf9K1N3AiB_A). (Amusing little tidbit: a friend bought Autoditacker on vinyl, and didn’t notice it was supposed to be spun at 45 RPM instead of 33 1/3. But at 33 1/3, it has a really dank, funky vibe to it, so we kept playing it like that for a while.)\n\n## B12\n\nKind of low-key, and heavily influenced by Detroit techno, their album [Time Tourist](https://open.spotify.com/album/1jLV4B5W9rjB8VcsyhyuNA?si=g2SGLEYZRhKInhe06ZsIjQ) was a go-to whenever I wanted some chill IDM. [Electro-Soma](https://open.spotify.com/album/4qDcCVpOsd8V4MsmqZrPr1?si=kWLufwFGT6m3i0McBy3y6w) is also pretty good, and collects a lot of their early EPs. Sometimes called themselves Musicology and Redcell.\n\n## Compilations\n\n- [Artificial Intelligence 1](https://open.spotify.com/album/34aK04FrYZGC4SMS3WEPIi?si=eRN-ypuxQ3C0wgx79oef5Q) and [2](https://open.spotify.com/album/76RSvTjwI1rjqnqBb2TUXq?si=YuLHXIaPR6quJRAa_ShZNQ) - the Warp compilations that announced the arrival of their sound. Features Aphex Twin (as Polygon Window), Autechre, B12 (as Musicology), The Black Dog (as IAO), and The Orb (as Dr Alex Patterson). Mix of more dance-y stuff along with the IDM they’d soon become known for.\n- [Warp10+3 Remixes](https://open.spotify.com/album/32tT9ncemGL0sjsYDoVTJb?si=av8Um4mMRuGnJ4dW5tAwEA) - for their tenth anniversary, Warp released a double album of everyone remixing each other\n- We Are Reasonable People - a later-period Warp compilation, features a good overview of the label from the late 90s\n\n## Others I liked\n\nThese were all artists I listened to then, but have fallen by the wayside over the past twenty years. Some were big, some were obscure.\n\n- Matmos - [A Chance to Cut is a Chance to Cure](https://open.spotify.com/album/1g9Acy5gJWRBbybCQyMPzV?si=w2YjHZd6R7eChD9f20WV8Q)\n- µ-Ziq - check out [Lunatic Harness](https://open.spotify.com/album/5Rc5G86a4zX96HlyHYeGsX?si=GRKrAM0KREywv2QNWOEuWA). His label, Planet Mu, released several IDM artists.\n- Arovane - bit like early Autechre\n- Pole - deep spacey minimalist dub, check out albums [3](https://open.spotify.com/album/5bWOl4J17QLviFkJYboxGL?si=ii0VCVz8Q22wdCtjmiJ8bw) and [1](https://open.spotify.com/album/0ybb3fO4tWtCzu4VKieRhL?si=Jsl4SCuJRQ-cntUDW2R7wA)\n- Future Sound of London - more on avant-techno/dance side\n- Plone - super-twee and cute, like Fisher-Price synths\n- The Orb - ambient house with a tinge of psychedelia, check out UFOrb and Orbus Terrarum\n- Plastikman - more on the techno side\n- LFO - more on the underground techno side, never released much material, but were very influential\n- Nightmares on Wax - more avant-techno, like LFO and Plastikman\n- Coil - dark, post-industrial\n\n### Others I never personally got into, but who knows, maybe you will, gentle reader?\n\nSquarepusher, Prefuse 73, Oval, Fennesz, Venetian Snares, Bochum Welt, Two Lone Swordsmen, Daedalus, Luke Vibert, Meat Beat Manifesto\n\n### Post-90s\n\nSome of this stuff isn’t quite IDM, but may have been influenced by it.\n\n- Solvent - love this guy, check out [Solvent City](https://open.spotify.com/album/305f4DdbHl5PeKnlOewMgx?si=HZNXSf27TJu07FfDeBvtbg)\n- Oneohtrix Point Never - love him, too!\n- Caribou\n- Ulrich Schnauss\n- Four Tet\n- Isan - same label as Solvent\n\nAlso, if anything I linked to in Spotify says it's not from the 90's, that's a damn lie. Spotify can't seem to stop increasing album years whenever something is remastered or deluxed, which is mega-annoying when trying to follow an artist's trajectory.\n\n## Oddities and Rarities\n\nOne thing the artists of the time had a fetish for, was embedding images in spectrograms of their music. Plaid, Aphex Twin, Autechre and Venetian Snares all snuck this into their work. See [Magnetic Mag](https://www.magneticmag.com/2012/08/the-aphex-face-visualizing-the-sound-spectrum/) for pics.\n\nAnother thing these artists had a fetish for, was unpronounceable track names. I can’t explain that other than to say it seemed more robot-like?\n\nIf you can track it down, Autechre made a great remix of Stereolab’s Miss Modular.\n\nAs mentioned above, Aphex Twin’s unreleased album Melodies From Mars is worth a download.\n\nAutechre once released a MiniDisc-only album as Gescom, designed to take advantage of the MiniDisc’s ability to shuffle without gaps (which most CD players couldn’t do at the time). They made 88 super-short tracks meant to be shuffled seamlessly so you never heard the same album twice.\n\nThat's all, hope you enjoyed it!","src/content/blog/2019-05-03-my-fav-idm.md","09e0491d70153d0f",{"html":131,"metadata":132},"\u003Cp>I promised a friend at a coffee shop I’d write up a list of my thoughts on IDM, aka “Intelligent Dance Music”. It only took a year, but here it is.\u003C/p>\n\u003Cp>The term “Intelligent Dance Music” emerged in the early 90s to describe a lot of the \u003Ca href=\"https://warp.net/\">Warp\u003C/a> label’s artists. At the time, Warp was aiming for a post-club, at-home kind of music. The idea was that the music had too many beats to be ambient, but was usually too rhythmically complex to actually be danced to. (While many of the artists involved dislike the term, no better term ever stuck. For a hot moment in the late 90’s, it seemed like it would be “electronica”, but that was broader in scope, and didn’t last.)\u003C/p>\n\u003Cp>Many of the artists seem to follow a certain progression: over time they become ever more experimental, and the music becomes more skittery and abstract. I \u003Cem>personally\u003C/em> love when artists successfully walk the fine line between ambient and dance, hooks and experiments, so I tend to favor most artists’ early/middle work. I personally believe that music should have a visceral component, that becoming too experimental robs it of something in favor of a head-heavy approach, and my reviews reflect that. But that’s just me.\u003C/p>\n\u003Cp>What follows are my personal favorite IDM artists and albums from that era, with random notes and asides.\u003C/p>\n\u003Ch2 id=\"aphex-twin\">Aphex Twin\u003C/h2>\n\u003Cp>Richard D. James is amazingly talented, but also a bit of a cheeky wanker. He once lived in an old bank vault and drove a tank around town.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/43s2fKRQsOSB6rSrxtAXKK?si=3dqIsPVrQHSOncZYy2xgxQ\">Richard D. James Album\u003C/a> - this is the one that kicked it off for me. Growing up in southern Virginia, I’d never heard anything like it. This is still my favorite era of Aphex Twin.\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/0VG7XLJ8gSynEQDVnpHNNU?si=gsi06xUsQSmzGhqivTO1EA\">…I Care Because You Do\u003C/a> - the album that preceded Richard D James, and similar in style. (Also check out the unreleased Melodies From Mars if you like this period.)\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/7aNclGRxTysfh6z0d8671k?si=wVQqkx0vQ0iPAKbN-5TVaQ\">Selected Ambient Works 85-92\u003C/a> - his first album is very danceable, since much of it was written when he was a teenage DJ.\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/17vHPMmoxN5B8cdhCDeMTe?si=Wpk-ldBiSweyRIcHb83vmQ\">Selected Ambient Works 2\u003C/a> - the album that made him famous - More ambient in tone, but with many interesting experiments that go way beyond Eno’s original ambient concepts.\u003C/li>\n\u003C/ul>\n\u003Cp>Aphex Twin had multiple aliases, including AFX and Polygon Window. His own label released a zillion Analog Bubblebath compilations.\u003C/p>\n\u003Ch2 id=\"boards-of-canada\">Boards of Canada\u003C/h2>\n\u003Cp>Despite the name, they’re a couple of Scottish dudes who made music for sitting around the campfire with their art collective. Apparently they were big fans of the Film Board of Canada.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/1vWnB0hYmluskQuzxwo25a?si=p5_2Ea94QxCZAX8gzciSqQ\">Music Has the Right to Children\u003C/a> - their first album, and their best. They coaxed a warmth rarely heard from electronic instruments.\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/0LDCQOBp7NFjkD1LbuusCG?si=25vz5CJJRFGVfHvumzjHIg\">Twoism\u003C/a> - an early EP off the Skam label, quite good\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"autechre\">Autechre\u003C/h2>\n\u003Cp>Why these guys weren’t bigger, I’ve never understood. Both Aphex and Boards of Canada achieved breakout success, but not Autechre. The rumor was their name was short for “Audio Technology Research”, but they claimed the name was formed starting with “au” and then bashing the keyboard, which makes more sense, given their song titles.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/4KROnLN6Didp0F97RXaW7a?si=ZleIiQ_kQh-j0K2Y9jMVQg\">Incunabula\u003C/a> - their first album, and one of their best.\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/7EfhvG3RwdhzXrFlkDVxg4?si=m2IXR3UTSU2wyRDxi7afEw\">Amber\u003C/a> - their followup album, and a bit more gentle rhythmically.\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/0ioIXXMV89w0qC39FpxYnL?si=f6iiopZ5TIe8GoEJ55W8Ow\">Tri Repetae\u003C/a> - pretty good\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/6UcShvT8JIKOaHj39k6A8f?si=nF4oeKGfRqeiitGobA6r2Q\">Anvil Vapre\u003C/a> - a harsh, almost metal version of their sound\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/31nThm3LyQVvrndqxhvjWl?si=E3d0O8zAQIqBYFMGfsp1Tw\">Garbage\u003C/a> - lush, more ambient, and one of the most beautiful things they ever created. I used to put this on after raves, and I forever associate it with driving back from one party the next morning in the rain while my friends snoozed in the car.\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/7zlbMdBS3J2YQRDuMMT9u4?si=RY43hCupQ5C4WGU2_8RNog\">LP5\u003C/a> - a transitional record. Some tracks are fantastic, like \u003Ca href=\"https://open.spotify.com/track/4tBC5l4AkqYfQ5CCx5IrKm?si=awCYIrUBTAS9kIMXRBqMug\">Rae\u003C/a> and \u003Ca href=\"https://open.spotify.com/track/770zYtbMuU5grFPUIzxmfQ?si=-IYACGkgRwKQP9QNSQbcrQ\">Corc\u003C/a>, but overall, this marks Autechre’s shift into more experimental realms\u003C/li>\n\u003C/ul>\n\u003Cp>Also worth hunting down are shadowy Gescom releases (rumored to be short for Gestalt Communications), which Ae frequently contributed to.\u003C/p>\n\u003Cp>Fun little fact: they released an album, \u003Cem>Anti\u003C/em>, in response to a law criminalizing parties in Britain with a “succession of repetitive beats” by removing all repetition. It came with a sticker attesting to its legality, but suggested you still have a lawyer present just in case.\u003C/p>\n\u003Ch2 id=\"plaid--the-black-dog\">Plaid / The Black Dog\u003C/h2>\n\u003Cp>While two different bands, they were originally one. Two of the founders of The Black Dog split off to make Plaid. They were associated with Bjork, and Plaid’s remix of her song, \u003Ca href=\"https://open.spotify.com/track/2ctHJYdYUsVPlJzhtQe6Jw?si=BEScs4pQQU6UOQqIvCFfzQ\">All Is Full of Love\u003C/a>, is well worth a listen.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/5O4LYrdtTr4nMnkn2GwP74?si=FKFg5GzzTyO_M5Jse5hoAg\">Not For Threes\u003C/a> - first album, standouts are \u003Ca href=\"https://open.spotify.com/track/70jjCa99FYeC4xvUmZy3Vf?si=Va_b0t42RR6AhpjZSVfmUA\">Myopia\u003C/a>, \u003Ca href=\"https://open.spotify.com/track/1sbTGYNgcr9YUSJoEY0oli?si=FZAxxmo5RdqJH7Ed6-ePyw\">Kortisin\u003C/a>, \u003Ca href=\"https://open.spotify.com/track/7FRNVeofO8OL094wfOM9Le?si=u-qWxwXaTEKtCTkYo_e9GA\">Prague Radio\u003C/a>, \u003Ca href=\"https://open.spotify.com/track/2ETmKAPTdY9DEIcWNo5IiZ?si=CHHPpNi3TtCTbPsBWQsUow\">Rakimou\u003C/a>, and \u003Ca href=\"https://open.spotify.com/track/3nSfJkSb7xRtrtJMKohXGc?si=_3K1WdAhRbev6ONR67tn_A\">Lilith\u003C/a>, which Bjork guest vocals for\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/4IkvCDzidC5XbElsN1SUsg?si=Qch2ygPKSnuCOxh8XFixoQ\">Rest-Proof Clockwork\u003C/a> - the follow-up, also pretty good\u003C/li>\n\u003Cli>The Digging Remedy - I was fairly luke-warm to a lot of their 2000’s output, but I recently listened to this and it was way better than I expected\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"mouse-on-mars\">Mouse on Mars\u003C/h2>\n\u003Cp>A couple of German fellows who crafted weird mixes of ambient, dub, and squiggly tunes. More playful than most artists of the era.\u003C/p>\n\u003Cp>Check out \u003Ca href=\"https://open.spotify.com/album/4Ev3UAEmQeMotvOS6w2CO1?si=Fsb4ucUzTSW2m9__OPYBUA\">Autoditacker\u003C/a> and \u003Ca href=\"https://open.spotify.com/album/73ilzyuAD5B5fhPekLrYiz?si=QMlkOuGlTaOf9K1N3AiB_A\">Niun Niggung\u003C/a>. (Amusing little tidbit: a friend bought Autoditacker on vinyl, and didn’t notice it was supposed to be spun at 45 RPM instead of 33 1/3. But at 33 1/3, it has a really dank, funky vibe to it, so we kept playing it like that for a while.)\u003C/p>\n\u003Ch2 id=\"b12\">B12\u003C/h2>\n\u003Cp>Kind of low-key, and heavily influenced by Detroit techno, their album \u003Ca href=\"https://open.spotify.com/album/1jLV4B5W9rjB8VcsyhyuNA?si=g2SGLEYZRhKInhe06ZsIjQ\">Time Tourist\u003C/a> was a go-to whenever I wanted some chill IDM. \u003Ca href=\"https://open.spotify.com/album/4qDcCVpOsd8V4MsmqZrPr1?si=kWLufwFGT6m3i0McBy3y6w\">Electro-Soma\u003C/a> is also pretty good, and collects a lot of their early EPs. Sometimes called themselves Musicology and Redcell.\u003C/p>\n\u003Ch2 id=\"compilations\">Compilations\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/34aK04FrYZGC4SMS3WEPIi?si=eRN-ypuxQ3C0wgx79oef5Q\">Artificial Intelligence 1\u003C/a> and \u003Ca href=\"https://open.spotify.com/album/76RSvTjwI1rjqnqBb2TUXq?si=YuLHXIaPR6quJRAa_ShZNQ\">2\u003C/a> - the Warp compilations that announced the arrival of their sound. Features Aphex Twin (as Polygon Window), Autechre, B12 (as Musicology), The Black Dog (as IAO), and The Orb (as Dr Alex Patterson). Mix of more dance-y stuff along with the IDM they’d soon become known for.\u003C/li>\n\u003Cli>\u003Ca href=\"https://open.spotify.com/album/32tT9ncemGL0sjsYDoVTJb?si=av8Um4mMRuGnJ4dW5tAwEA\">Warp10+3 Remixes\u003C/a> - for their tenth anniversary, Warp released a double album of everyone remixing each other\u003C/li>\n\u003Cli>We Are Reasonable People - a later-period Warp compilation, features a good overview of the label from the late 90s\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"others-i-liked\">Others I liked\u003C/h2>\n\u003Cp>These were all artists I listened to then, but have fallen by the wayside over the past twenty years. Some were big, some were obscure.\u003C/p>\n\u003Cul>\n\u003Cli>Matmos - \u003Ca href=\"https://open.spotify.com/album/1g9Acy5gJWRBbybCQyMPzV?si=w2YjHZd6R7eChD9f20WV8Q\">A Chance to Cut is a Chance to Cure\u003C/a>\u003C/li>\n\u003Cli>µ-Ziq - check out \u003Ca href=\"https://open.spotify.com/album/5Rc5G86a4zX96HlyHYeGsX?si=GRKrAM0KREywv2QNWOEuWA\">Lunatic Harness\u003C/a>. His label, Planet Mu, released several IDM artists.\u003C/li>\n\u003Cli>Arovane - bit like early Autechre\u003C/li>\n\u003Cli>Pole - deep spacey minimalist dub, check out albums \u003Ca href=\"https://open.spotify.com/album/5bWOl4J17QLviFkJYboxGL?si=ii0VCVz8Q22wdCtjmiJ8bw\">3\u003C/a> and \u003Ca href=\"https://open.spotify.com/album/0ybb3fO4tWtCzu4VKieRhL?si=Jsl4SCuJRQ-cntUDW2R7wA\">1\u003C/a>\u003C/li>\n\u003Cli>Future Sound of London - more on avant-techno/dance side\u003C/li>\n\u003Cli>Plone - super-twee and cute, like Fisher-Price synths\u003C/li>\n\u003Cli>The Orb - ambient house with a tinge of psychedelia, check out UFOrb and Orbus Terrarum\u003C/li>\n\u003Cli>Plastikman - more on the techno side\u003C/li>\n\u003Cli>LFO - more on the underground techno side, never released much material, but were very influential\u003C/li>\n\u003Cli>Nightmares on Wax - more avant-techno, like LFO and Plastikman\u003C/li>\n\u003Cli>Coil - dark, post-industrial\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"others-i-never-personally-got-into-but-who-knows-maybe-you-will-gentle-reader\">Others I never personally got into, but who knows, maybe you will, gentle reader?\u003C/h3>\n\u003Cp>Squarepusher, Prefuse 73, Oval, Fennesz, Venetian Snares, Bochum Welt, Two Lone Swordsmen, Daedalus, Luke Vibert, Meat Beat Manifesto\u003C/p>\n\u003Ch3 id=\"post-90s\">Post-90s\u003C/h3>\n\u003Cp>Some of this stuff isn’t quite IDM, but may have been influenced by it.\u003C/p>\n\u003Cul>\n\u003Cli>Solvent - love this guy, check out \u003Ca href=\"https://open.spotify.com/album/305f4DdbHl5PeKnlOewMgx?si=HZNXSf27TJu07FfDeBvtbg\">Solvent City\u003C/a>\u003C/li>\n\u003Cli>Oneohtrix Point Never - love him, too!\u003C/li>\n\u003Cli>Caribou\u003C/li>\n\u003Cli>Ulrich Schnauss\u003C/li>\n\u003Cli>Four Tet\u003C/li>\n\u003Cli>Isan - same label as Solvent\u003C/li>\n\u003C/ul>\n\u003Cp>Also, if anything I linked to in Spotify says it’s not from the 90’s, that’s a damn lie. Spotify can’t seem to stop increasing album years whenever something is remastered or deluxed, which is mega-annoying when trying to follow an artist’s trajectory.\u003C/p>\n\u003Ch2 id=\"oddities-and-rarities\">Oddities and Rarities\u003C/h2>\n\u003Cp>One thing the artists of the time had a fetish for, was embedding images in spectrograms of their music. Plaid, Aphex Twin, Autechre and Venetian Snares all snuck this into their work. See \u003Ca href=\"https://www.magneticmag.com/2012/08/the-aphex-face-visualizing-the-sound-spectrum/\">Magnetic Mag\u003C/a> for pics.\u003C/p>\n\u003Cp>Another thing these artists had a fetish for, was unpronounceable track names. I can’t explain that other than to say it seemed more robot-like?\u003C/p>\n\u003Cp>If you can track it down, Autechre made a great remix of Stereolab’s Miss Modular.\u003C/p>\n\u003Cp>As mentioned above, Aphex Twin’s unreleased album Melodies From Mars is worth a download.\u003C/p>\n\u003Cp>Autechre once released a MiniDisc-only album as Gescom, designed to take advantage of the MiniDisc’s ability to shuffle without gaps (which most CD players couldn’t do at the time). They made 88 super-short tracks meant to be shuffled seamlessly so you never heard the same album twice.\u003C/p>\n\u003Cp>That’s all, hope you enjoyed it!\u003C/p>",{"headings":133,"localImagePaths":164,"remoteImagePaths":165,"frontmatter":166,"imagePaths":169},[134,136,138,140,143,146,149,152,155,158,161],{"depth":43,"slug":135,"text":123},"aphex-twin",{"depth":43,"slug":137,"text":125},"boards-of-canada",{"depth":43,"slug":139,"text":124},"autechre",{"depth":43,"slug":141,"text":142},"plaid--the-black-dog","Plaid / The Black Dog",{"depth":43,"slug":144,"text":145},"mouse-on-mars","Mouse on Mars",{"depth":43,"slug":147,"text":148},"b12","B12",{"depth":43,"slug":150,"text":151},"compilations","Compilations",{"depth":43,"slug":153,"text":154},"others-i-liked","Others I liked",{"depth":47,"slug":156,"text":157},"others-i-never-personally-got-into-but-who-knows-maybe-you-will-gentle-reader","Others I never personally got into, but who knows, maybe you will, gentle reader?",{"depth":47,"slug":159,"text":160},"post-90s","Post-90s",{"depth":43,"slug":162,"text":163},"oddities-and-rarities","Oddities and Rarities",[],[],{"title":117,"pubDate":167,"tags":168,"description":118},["Date","2019-05-03T00:00:00.000Z"],[121,122,123,124,125,126],[],"2023-06-20-aleph-http2-changes",{"id":170,"data":172,"body":180,"filePath":181,"digest":182,"rendered":183},{"title":173,"description":174,"pubDate":175,"tags":176},"Adding HTTP/2 client support to Aleph","A technical overview of implementing HTTP/2 client support in Aleph, detailing challenges with stream handling, protocol negotiation, and real-world implementation differences from the HTTP/2 specification.",["Date","2023-06-20T00:00:00.000Z"],[177,178,18,179],"Aleph","HTTP/2","HTTP","Earlier this year, I received a grant from Clojurists Together to modernize Aleph and bring HTTP/2 support to it. These are some of the interesting tidbits of the journey so far.\n\n## HTTP/2: Now Twice as Hypertextual!\n\nI began with a deep dive into [RFC 9113](https://www.rfc-editor.org/rfc/rfc9113.html) and its predecessors, and absorbed everything I could about the new updates for [HTTP/2](https://web.dev/performance-http2/): streams, frames, flow control, server push, prioritization, connection status, pseudo-headers, etc. Some of the underlying changes are handled invisibly by Netty (Aleph's underlying network lib), like the new binary frames and header compression, but the rest required serious updates to Aleph to get working.\n\n### Streams\n\nHTTP is based on the simple concept of: send a request, wait for a response, repeat. Unfortunately, having to wait for the previous response before getting to send the next request is suboptimal. In theory, [HTTP/1.1 supports pipelining](https://developer.mozilla.org/en-US/docs/Web/HTTP/Connection_management_in_HTTP_1.x#http_pipelining), where you can fire off multiple requests without waiting for their responses, and eventually get the responses back in the same order. In reality, that breaks intermediate caches and proxies, so it's rarely enabled. The general solution is to just open multiple TCP connections to the server. Browsers typically limit it to a maximum of six connections per server, putting a cap on parallelism.\n\nHTTP/2 solves this by assigning each request/response pair to a unique *stream* (not to be confused with Manifold streams), allowing you to tell which frames are related and safely interleave them. Unfortunately, this broke Aleph's deep assumptions about how things worked. \n\nWhen you make a request, Aleph returns a Manifold deferred (like a `CompletableFuture`), but under the hood, it transforms the Ring request map, places it on a `requests` Manifold stream, then a Manifold consumption callback pulls requests off the stream, transforms them some more, converts for Netty, and then places them on the Netty pipeline.\n\nEach `put!` on the `requests` stream is followed by its corresponding `take!` from the `responses` stream. This works under the assumptions of HTTP/1 order, but breaks as soon as you have multiple requests in flight with HTTP/2, since an incoming response could be for an out-of-order request.\n\nFixing this required removing much of the underlying Manifold stream code for HTTP/2 connections.\n\n### HTTP Version Negotiation\n\nSupporting multiple HTTP versions requires changes to how how SSL/TLS is used. With just HTTP/1, you can connect to a web server and start transmitting. But RFC 9113 requires that you use TLS's Application-Layer Protocol Negotiation (ALPN; [RFC 7301](https://datatracker.ietf.org/doc/html/rfc7301)) with secure HTTP/2. \n\nIn essence, the client and server agree on a protocol during the TLS handshake process. This is handy, but it broke Aleph's setup process, since you can no longer set up the Manifold code and Netty pipeline for a TCP connection in advance. \n\nWhat about *insecure* HTTP/2? Aleph doesn't support it yet, but the spec allows it if you *know in advance* that the server supports it (i.e., servers you control in your own internal network).\n\n### Lies, Damned Lies, and Specifications\n\nI learned a ton of material from the RFCs and design documents, and then I promptly had to *unlearn* a quarter of it. With evolving specs, there's a real danger in reading outdated information. It's not as simple as ignoring old RFCs, either.\n\n##### Server Push\n\nSee, the RFC doesn't always reflect reality. Server push, where the server can initiate \"responses\" that the client hasn't requested (yet), turned out to be extremely difficult to get right. To truly do it correctly requires understanding the network timings of the connection, low-level control of the OS's TCP/IP buffers, and be able to interrogate the browser's cache. Done badly, it will actually make things *slower*. Chrome [effectively disabled it](https://developer.chrome.com/blog/removing-push/) last year, by turning it off for all new connections, but that's not in the specs.\n\n##### Prioritization\n\nLikewise, prioritization [did not pan out as hoped](https://blog.cloudflare.com/better-http-2-prioritization-for-a-faster-web/). In earlier versions of the HTTP/2 RFCs, they described a system relating streams to each other in a weighted DAG of dependencies. Unfortunately, each browser had different ideas of how to interpret the weights; Safari and Edge effectively ignored prioritization entirely. This led to servers being unable to use it in a general manner. On top of that, browsers already prioritized how they ordered/delayed requests to begin with. In the most recent RFC, prioritization was deprecated; for HTTP/3, they've started promoting a simpler, header-based prioritization system ([RFC 9218](https://www.rfc-editor.org/rfc/rfc9218.html)), which is backwards-compatible, so we hope to backport it when HTTP/3 work begins.\n\n### Next Steps\n\nThe client code is under review now, and will be available as an alpha preview soon.\n\nMany thanks to [Clojurists Together](https://www.clojuriststogether.org/) for supporting this work.","src/content/blog/2023-06-20-aleph-http2-changes.md","9b3e5d0ca2d4167b",{"html":184,"metadata":185},"\u003Cp>Earlier this year, I received a grant from Clojurists Together to modernize Aleph and bring HTTP/2 support to it. These are some of the interesting tidbits of the journey so far.\u003C/p>\n\u003Ch2 id=\"http2-now-twice-as-hypertextual\">HTTP/2: Now Twice as Hypertextual!\u003C/h2>\n\u003Cp>I began with a deep dive into \u003Ca href=\"https://www.rfc-editor.org/rfc/rfc9113.html\">RFC 9113\u003C/a> and its predecessors, and absorbed everything I could about the new updates for \u003Ca href=\"https://web.dev/performance-http2/\">HTTP/2\u003C/a>: streams, frames, flow control, server push, prioritization, connection status, pseudo-headers, etc. Some of the underlying changes are handled invisibly by Netty (Aleph’s underlying network lib), like the new binary frames and header compression, but the rest required serious updates to Aleph to get working.\u003C/p>\n\u003Ch3 id=\"streams\">Streams\u003C/h3>\n\u003Cp>HTTP is based on the simple concept of: send a request, wait for a response, repeat. Unfortunately, having to wait for the previous response before getting to send the next request is suboptimal. In theory, \u003Ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Connection_management_in_HTTP_1.x#http_pipelining\">HTTP/1.1 supports pipelining\u003C/a>, where you can fire off multiple requests without waiting for their responses, and eventually get the responses back in the same order. In reality, that breaks intermediate caches and proxies, so it’s rarely enabled. The general solution is to just open multiple TCP connections to the server. Browsers typically limit it to a maximum of six connections per server, putting a cap on parallelism.\u003C/p>\n\u003Cp>HTTP/2 solves this by assigning each request/response pair to a unique \u003Cem>stream\u003C/em> (not to be confused with Manifold streams), allowing you to tell which frames are related and safely interleave them. Unfortunately, this broke Aleph’s deep assumptions about how things worked.\u003C/p>\n\u003Cp>When you make a request, Aleph returns a Manifold deferred (like a \u003Ccode>CompletableFuture\u003C/code>), but under the hood, it transforms the Ring request map, places it on a \u003Ccode>requests\u003C/code> Manifold stream, then a Manifold consumption callback pulls requests off the stream, transforms them some more, converts for Netty, and then places them on the Netty pipeline.\u003C/p>\n\u003Cp>Each \u003Ccode>put!\u003C/code> on the \u003Ccode>requests\u003C/code> stream is followed by its corresponding \u003Ccode>take!\u003C/code> from the \u003Ccode>responses\u003C/code> stream. This works under the assumptions of HTTP/1 order, but breaks as soon as you have multiple requests in flight with HTTP/2, since an incoming response could be for an out-of-order request.\u003C/p>\n\u003Cp>Fixing this required removing much of the underlying Manifold stream code for HTTP/2 connections.\u003C/p>\n\u003Ch3 id=\"http-version-negotiation\">HTTP Version Negotiation\u003C/h3>\n\u003Cp>Supporting multiple HTTP versions requires changes to how how SSL/TLS is used. With just HTTP/1, you can connect to a web server and start transmitting. But RFC 9113 requires that you use TLS’s Application-Layer Protocol Negotiation (ALPN; \u003Ca href=\"https://datatracker.ietf.org/doc/html/rfc7301\">RFC 7301\u003C/a>) with secure HTTP/2.\u003C/p>\n\u003Cp>In essence, the client and server agree on a protocol during the TLS handshake process. This is handy, but it broke Aleph’s setup process, since you can no longer set up the Manifold code and Netty pipeline for a TCP connection in advance.\u003C/p>\n\u003Cp>What about \u003Cem>insecure\u003C/em> HTTP/2? Aleph doesn’t support it yet, but the spec allows it if you \u003Cem>know in advance\u003C/em> that the server supports it (i.e., servers you control in your own internal network).\u003C/p>\n\u003Ch3 id=\"lies-damned-lies-and-specifications\">Lies, Damned Lies, and Specifications\u003C/h3>\n\u003Cp>I learned a ton of material from the RFCs and design documents, and then I promptly had to \u003Cem>unlearn\u003C/em> a quarter of it. With evolving specs, there’s a real danger in reading outdated information. It’s not as simple as ignoring old RFCs, either.\u003C/p>\n\u003Ch5 id=\"server-push\">Server Push\u003C/h5>\n\u003Cp>See, the RFC doesn’t always reflect reality. Server push, where the server can initiate “responses” that the client hasn’t requested (yet), turned out to be extremely difficult to get right. To truly do it correctly requires understanding the network timings of the connection, low-level control of the OS’s TCP/IP buffers, and be able to interrogate the browser’s cache. Done badly, it will actually make things \u003Cem>slower\u003C/em>. Chrome \u003Ca href=\"https://developer.chrome.com/blog/removing-push/\">effectively disabled it\u003C/a> last year, by turning it off for all new connections, but that’s not in the specs.\u003C/p>\n\u003Ch5 id=\"prioritization\">Prioritization\u003C/h5>\n\u003Cp>Likewise, prioritization \u003Ca href=\"https://blog.cloudflare.com/better-http-2-prioritization-for-a-faster-web/\">did not pan out as hoped\u003C/a>. In earlier versions of the HTTP/2 RFCs, they described a system relating streams to each other in a weighted DAG of dependencies. Unfortunately, each browser had different ideas of how to interpret the weights; Safari and Edge effectively ignored prioritization entirely. This led to servers being unable to use it in a general manner. On top of that, browsers already prioritized how they ordered/delayed requests to begin with. In the most recent RFC, prioritization was deprecated; for HTTP/3, they’ve started promoting a simpler, header-based prioritization system (\u003Ca href=\"https://www.rfc-editor.org/rfc/rfc9218.html\">RFC 9218\u003C/a>), which is backwards-compatible, so we hope to backport it when HTTP/3 work begins.\u003C/p>\n\u003Ch3 id=\"next-steps\">Next Steps\u003C/h3>\n\u003Cp>The client code is under review now, and will be available as an alpha preview soon.\u003C/p>\n\u003Cp>Many thanks to \u003Ca href=\"https://www.clojuriststogether.org/\">Clojurists Together\u003C/a> for supporting this work.\u003C/p>",{"headings":186,"localImagePaths":209,"remoteImagePaths":210,"frontmatter":211,"imagePaths":214},[187,190,193,196,199,203,206],{"depth":43,"slug":188,"text":189},"http2-now-twice-as-hypertextual","HTTP/2: Now Twice as Hypertextual!",{"depth":47,"slug":191,"text":192},"streams","Streams",{"depth":47,"slug":194,"text":195},"http-version-negotiation","HTTP Version Negotiation",{"depth":47,"slug":197,"text":198},"lies-damned-lies-and-specifications","Lies, Damned Lies, and Specifications",{"depth":200,"slug":201,"text":202},5,"server-push","Server Push",{"depth":200,"slug":204,"text":205},"prioritization","Prioritization",{"depth":47,"slug":207,"text":208},"next-steps","Next Steps",[],[],{"title":173,"pubDate":212,"tags":213,"description":174},["Date","2023-06-20T00:00:00.000Z"],[177,178,18,179],[],"2023-03-17-undertale-exapunks",{"id":215,"data":217,"body":225,"filePath":226,"digest":227,"rendered":228},{"title":218,"description":219,"pubDate":220,"badge":221,"tags":222},"Undertale in EXAPUNKS","My mashup demo recreating Undertale inside the Zachtronics' game EXAPUNKS.",["Date","2023-03-17T00:00:00.000Z"],"Fun!",[223,224],"Undertale","Exapunks","\u003Ciframe class=\"aspect-[560/315] w-full\" src=\"https://www.youtube.com/embed/pGUDeObiS38?si=m3MXznWSHee-QN_L\" title=\"Undertale EXAPUNKS\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen>\u003C/iframe>\n\nI play games pretty slowly these days. Never finished Myst. Still working on Cuphead.\n\nMy wife and I played [Undertale](https://undertale.com/) a couple years after it came out, and loved it. \n\nI started playing [Zachtronics](https://www.zachtronics.com/)'s [EXAPUNKS](https://www.zachtronics.com/exapunks/) a few years ago, and finally beat it last fall, only to hear that Zachtronics was closing down and moving on *(UPDATE: The Zachtronics games are still available, and Zach eventually returned to making games with his new company, [Coincidence](https://coincidence.games/).)* EXAPUNKS is essentially \"Assembly language: the game\" meets Neuromancer. After a long day of high-level abstraction in your code, sometimes you just want to twiddle some bits.\n\nOne of the missions involves hacking a Virtual Boy-like game system, but you can also make freeform games for it, so I made this \"demo\" of Undertale's Asriel battle in EXAPUNKS. \n\nFor my own amusement, I'm pretending to have \"found\" it while hopping dimensions; we'll see if anyone's fooled. \n\nEnjoy!","src/content/blog/2023-03-17-undertale-exapunks.md","61838778e53f4d82",{"html":229,"metadata":230},"\u003Ciframe class=\"aspect-[560/315] w-full\" src=\"https://www.youtube.com/embed/pGUDeObiS38?si=m3MXznWSHee-QN_L\" title=\"Undertale EXAPUNKS\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen>\u003C/iframe>\n\u003Cp>I play games pretty slowly these days. Never finished Myst. Still working on Cuphead.\u003C/p>\n\u003Cp>My wife and I played \u003Ca href=\"https://undertale.com/\">Undertale\u003C/a> a couple years after it came out, and loved it.\u003C/p>\n\u003Cp>I started playing \u003Ca href=\"https://www.zachtronics.com/\">Zachtronics\u003C/a>’s \u003Ca href=\"https://www.zachtronics.com/exapunks/\">EXAPUNKS\u003C/a> a few years ago, and finally beat it last fall, only to hear that Zachtronics was closing down and moving on \u003Cem>(UPDATE: The Zachtronics games are still available, and Zach eventually returned to making games with his new company, \u003Ca href=\"https://coincidence.games/\">Coincidence\u003C/a>.)\u003C/em> EXAPUNKS is essentially “Assembly language: the game” meets Neuromancer. After a long day of high-level abstraction in your code, sometimes you just want to twiddle some bits.\u003C/p>\n\u003Cp>One of the missions involves hacking a Virtual Boy-like game system, but you can also make freeform games for it, so I made this “demo” of Undertale’s Asriel battle in EXAPUNKS.\u003C/p>\n\u003Cp>For my own amusement, I’m pretending to have “found” it while hopping dimensions; we’ll see if anyone’s fooled.\u003C/p>\n\u003Cp>Enjoy!\u003C/p>",{"headings":231,"localImagePaths":232,"remoteImagePaths":233,"frontmatter":234,"imagePaths":237},[],[],[],{"title":218,"pubDate":235,"tags":236,"badge":221,"description":219},["Date","2023-03-17T00:00:00.000Z"],[223,224],[],"2025-03-04-test-impact-analysis",{"id":238,"data":240,"body":249,"filePath":250,"digest":251,"rendered":252},{"title":241,"description":242,"pubDate":243,"tags":244},"You Should Test Less - Skip Irrelevant Tests With Impact Analysis","When we make a change to code, we should only run tests that can be proven to be relevant. Running unrelated tests tells you nothing useful, wastes electrici...",["Date","2025-03-04T00:00:00.000Z"],[245,246,247,248,87,19],"testing","devops","TIA","test-impact analysis","When we make a change to code, we should only run tests that can be proven to be relevant. Running unrelated tests tells you nothing useful, wastes electricity, and makes your scrum master cry.\n\nSo why do we run irrelevant tests to begin with? Our testing habits came from unit tests, where the goal is to make tests so fast, you can rerun them all frequently. But this broke down as test suites got longer and slower.\n\nI have personally seen CI for PRs take up to 120 minutes to complete, and full end-to-end tests so slow they could only be run overnight.\n\nIf your tests finish in under 15 minutes, great! ...this post isn't for you, close the tab. \n\n## What can we do instead of pulling our hair out?\nThere's a solution that deserves to be better-known outside FAANGs: **test-impact analysis (TIA)**. With TIA, you use code and file dependencies to skip tests that cannot be affected by changes in a PR.\n\nTIA is not new. Variants of it are used in Google's [TAP](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45861.pdf). Jest and Vitest support it. Microsoft coined the term itself, and [offers it](https://learn.microsoft.com/en-us/azure/devops/pipelines/test/test-impact-analysis?view=azure-devops) in Azure DevOps. Thought leaders [have thought about it](https://martinfowler.com/articles/rise-test-impact-analysis.html).\n\n## How does test-impact analysis work?\nYou might already have a pitchfork in hand, shouting \"It's unsafe to skip tests!\"\n\nBut done correctly, TIA will only skip tests it *knows* are irrelevant. You can trust it as much as you trust your compiler (and some of the TIA methods are the same as your compiler's).\n\n### Automatic\nAutomatic TIA methods use static code analysis or run-time coverage to figure out dependencies. You don't have to change anything with your tests for them to work. However, they don't usually understand non-code changes, so they run all tests to be safe in that case.\n\n#### 1. **File dependencies**  \n   File dependencies are fast, safe, and simple, but can \"overtest\". Not every test in a file may be affected by a change in a depended-on code file. It's best for codebases that don't import/export more than they need to (e.g., Python `__init__.py` files that import everything under the sun.)\n\n#### 2. **Program dependency graphs**  \n   Program dependency graphs are what compilers use in optimization, but they work for TIA, too. By analyzing data dependencies and control flow, they can match up tests to individual lines of code. They won't overtest, though they still have to be cautious when non-code files change.\n\n#### 3. **Coverage**  \n   Coverage can map code dependencies that aren't amenable to static analysis, like which branch of an if a test always takes.  \n   This comes with a downside, though; to collect coverage info requires a full test run to start. If you want to use TIA in a fresh CI environment, you need to run all tests beforehand, store that coverage data, and share it with CI. (If you don't, CI has to run the full test suite, defeating the whole purpose.) Coverage methods work well, at the cost of more complexity, and harder CI integration.\n\n### Manual\nIn manual specification, you write out all the dependencies yourself. (This is more common in full build systems like Bazel.)\nThe major advantage is you can use TIA for *non*-code changes.\n\nBut the major disadvantage is that unlike the automatic methods, which can't miss tests, if you specify the dependencies wrong, you can.\n\n## Complementary speed-up methods\nTIA is safe, cheap, requires minimal changes, and combines well with other acceleration methods.\n\n#### 1. **Test suites**  \n   The traditional speed-up approach divides tests into suites. When updating front-end-related code, run only the front-end suite. While simple, this method is coarse and needs manual setup.\n\n#### 2. **Parallelization**  \n   Parallelization is very effective, but has several gotchas. Tests must run safely in parallel, without inter-test dependencies. Shared resources (networks, databases) must handle concurrent access or be replicated. Manual parallelization is labor-intensive, while automated approaches requires checking to ensure no subtle heisenbugs. \n\n#### 3. **Predictive test selection**  \n   With this, you choose tests based on how likely they are to fail. This is mostly useful for FAANGs that can't run all relevant tests for each PR. And it doesn't eliminate their need to run all tests periodically anyway. You probably don't need predictive test selection just yet.\n\n   Predictive selection can be as simple as choosing the top-_n_ failing tests, or as sophisticated as building [a machine-learning model that predicts relevant tests](https://engineering.fb.com/2018/11/21/developer-tools/predictive-test-selection/) from code changes.\n\n#### 4. **LLMs**  \n   Since it's 2025, you *can* ask ChatGPT to select relevant tests, but this is fuzzy. It probably misses relevant tests sometimes and runs irrelevant ones. That being said, I suspect LLM/ML techniques might do a decent job of mapping tests to changes in non-code files, but I'm not sure I'd trust it with the keys to the car just yet.\n\n### Try it out on your tests! Or don't, I'm not your mom.\nIf you're sick of waiting on CI tests to finish, give test-impact analysis a try. If you consume too much coffee already, and don't want another excuse to get a refill while you wait, give TIA a try. If you're the sort who always turn off the light when you exit the room to save energy, definitely try TIA.\n\n\n----\n\n[^faang-scale]: In Google's TAP paper linked above, they say they deal with 1 commit _per second_.","src/content/blog/2025-03-04-test-impact-analysis.md","5f1865b7c88f3a25",{"html":253,"metadata":254},"\u003Cp>When we make a change to code, we should only run tests that can be proven to be relevant. Running unrelated tests tells you nothing useful, wastes electricity, and makes your scrum master cry.\u003C/p>\n\u003Cp>So why do we run irrelevant tests to begin with? Our testing habits came from unit tests, where the goal is to make tests so fast, you can rerun them all frequently. But this broke down as test suites got longer and slower.\u003C/p>\n\u003Cp>I have personally seen CI for PRs take up to 120 minutes to complete, and full end-to-end tests so slow they could only be run overnight.\u003C/p>\n\u003Cp>If your tests finish in under 15 minutes, great! …this post isn’t for you, close the tab.\u003C/p>\n\u003Ch2 id=\"what-can-we-do-instead-of-pulling-our-hair-out\">What can we do instead of pulling our hair out?\u003C/h2>\n\u003Cp>There’s a solution that deserves to be better-known outside FAANGs: \u003Cstrong>test-impact analysis (TIA)\u003C/strong>. With TIA, you use code and file dependencies to skip tests that cannot be affected by changes in a PR.\u003C/p>\n\u003Cp>TIA is not new. Variants of it are used in Google’s \u003Ca href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45861.pdf\">TAP\u003C/a>. Jest and Vitest support it. Microsoft coined the term itself, and \u003Ca href=\"https://learn.microsoft.com/en-us/azure/devops/pipelines/test/test-impact-analysis?view=azure-devops\">offers it\u003C/a> in Azure DevOps. Thought leaders \u003Ca href=\"https://martinfowler.com/articles/rise-test-impact-analysis.html\">have thought about it\u003C/a>.\u003C/p>\n\u003Ch2 id=\"how-does-test-impact-analysis-work\">How does test-impact analysis work?\u003C/h2>\n\u003Cp>You might already have a pitchfork in hand, shouting “It’s unsafe to skip tests!”\u003C/p>\n\u003Cp>But done correctly, TIA will only skip tests it \u003Cem>knows\u003C/em> are irrelevant. You can trust it as much as you trust your compiler (and some of the TIA methods are the same as your compiler’s).\u003C/p>\n\u003Ch3 id=\"automatic\">Automatic\u003C/h3>\n\u003Cp>Automatic TIA methods use static code analysis or run-time coverage to figure out dependencies. You don’t have to change anything with your tests for them to work. However, they don’t usually understand non-code changes, so they run all tests to be safe in that case.\u003C/p>\n\u003Ch4 id=\"1-file-dependencies\">1. \u003Cstrong>File dependencies\u003C/strong>\u003C/h4>\n\u003Cp>File dependencies are fast, safe, and simple, but can “overtest”. Not every test in a file may be affected by a change in a depended-on code file. It’s best for codebases that don’t import/export more than they need to (e.g., Python \u003Ccode>__init__.py\u003C/code> files that import everything under the sun.)\u003C/p>\n\u003Ch4 id=\"2-program-dependency-graphs\">2. \u003Cstrong>Program dependency graphs\u003C/strong>\u003C/h4>\n\u003Cp>Program dependency graphs are what compilers use in optimization, but they work for TIA, too. By analyzing data dependencies and control flow, they can match up tests to individual lines of code. They won’t overtest, though they still have to be cautious when non-code files change.\u003C/p>\n\u003Ch4 id=\"3-coverage\">3. \u003Cstrong>Coverage\u003C/strong>\u003C/h4>\n\u003Cp>Coverage can map code dependencies that aren’t amenable to static analysis, like which branch of an if a test always takes.\u003Cbr>\nThis comes with a downside, though; to collect coverage info requires a full test run to start. If you want to use TIA in a fresh CI environment, you need to run all tests beforehand, store that coverage data, and share it with CI. (If you don’t, CI has to run the full test suite, defeating the whole purpose.) Coverage methods work well, at the cost of more complexity, and harder CI integration.\u003C/p>\n\u003Ch3 id=\"manual\">Manual\u003C/h3>\n\u003Cp>In manual specification, you write out all the dependencies yourself. (This is more common in full build systems like Bazel.)\nThe major advantage is you can use TIA for \u003Cem>non\u003C/em>-code changes.\u003C/p>\n\u003Cp>But the major disadvantage is that unlike the automatic methods, which can’t miss tests, if you specify the dependencies wrong, you can.\u003C/p>\n\u003Ch2 id=\"complementary-speed-up-methods\">Complementary speed-up methods\u003C/h2>\n\u003Cp>TIA is safe, cheap, requires minimal changes, and combines well with other acceleration methods.\u003C/p>\n\u003Ch4 id=\"1-test-suites\">1. \u003Cstrong>Test suites\u003C/strong>\u003C/h4>\n\u003Cp>The traditional speed-up approach divides tests into suites. When updating front-end-related code, run only the front-end suite. While simple, this method is coarse and needs manual setup.\u003C/p>\n\u003Ch4 id=\"2-parallelization\">2. \u003Cstrong>Parallelization\u003C/strong>\u003C/h4>\n\u003Cp>Parallelization is very effective, but has several gotchas. Tests must run safely in parallel, without inter-test dependencies. Shared resources (networks, databases) must handle concurrent access or be replicated. Manual parallelization is labor-intensive, while automated approaches requires checking to ensure no subtle heisenbugs.\u003C/p>\n\u003Ch4 id=\"3-predictive-test-selection\">3. \u003Cstrong>Predictive test selection\u003C/strong>\u003C/h4>\n\u003Cp>With this, you choose tests based on how likely they are to fail. This is mostly useful for FAANGs that can’t run all relevant tests for each PR. And it doesn’t eliminate their need to run all tests periodically anyway. You probably don’t need predictive test selection just yet.\u003C/p>\n\u003Cp>Predictive selection can be as simple as choosing the top-\u003Cem>n\u003C/em> failing tests, or as sophisticated as building \u003Ca href=\"https://engineering.fb.com/2018/11/21/developer-tools/predictive-test-selection/\">a machine-learning model that predicts relevant tests\u003C/a> from code changes.\u003C/p>\n\u003Ch4 id=\"4-llms\">4. \u003Cstrong>LLMs\u003C/strong>\u003C/h4>\n\u003Cp>Since it’s 2025, you \u003Cem>can\u003C/em> ask ChatGPT to select relevant tests, but this is fuzzy. It probably misses relevant tests sometimes and runs irrelevant ones. That being said, I suspect LLM/ML techniques might do a decent job of mapping tests to changes in non-code files, but I’m not sure I’d trust it with the keys to the car just yet.\u003C/p>\n\u003Ch3 id=\"try-it-out-on-your-tests-or-dont-im-not-your-mom\">Try it out on your tests! Or don’t, I’m not your mom.\u003C/h3>\n\u003Cp>If you’re sick of waiting on CI tests to finish, give test-impact analysis a try. If you consume too much coffee already, and don’t want another excuse to get a refill while you wait, give TIA a try. If you’re the sort who always turn off the light when you exit the room to save energy, definitely try TIA.\u003C/p>\n\u003Chr>",{"headings":255,"localImagePaths":296,"remoteImagePaths":297,"frontmatter":298,"imagePaths":301},[256,259,262,265,269,272,275,278,281,284,287,290,293],{"depth":43,"slug":257,"text":258},"what-can-we-do-instead-of-pulling-our-hair-out","What can we do instead of pulling our hair out?",{"depth":43,"slug":260,"text":261},"how-does-test-impact-analysis-work","How does test-impact analysis work?",{"depth":47,"slug":263,"text":264},"automatic","Automatic",{"depth":266,"slug":267,"text":268},4,"1-file-dependencies","1. File dependencies",{"depth":266,"slug":270,"text":271},"2-program-dependency-graphs","2. Program dependency graphs",{"depth":266,"slug":273,"text":274},"3-coverage","3. Coverage",{"depth":47,"slug":276,"text":277},"manual","Manual",{"depth":43,"slug":279,"text":280},"complementary-speed-up-methods","Complementary speed-up methods",{"depth":266,"slug":282,"text":283},"1-test-suites","1. Test suites",{"depth":266,"slug":285,"text":286},"2-parallelization","2. Parallelization",{"depth":266,"slug":288,"text":289},"3-predictive-test-selection","3. Predictive test selection",{"depth":266,"slug":291,"text":292},"4-llms","4. LLMs",{"depth":47,"slug":294,"text":295},"try-it-out-on-your-tests-or-dont-im-not-your-mom","Try it out on your tests! Or don’t, I’m not your mom.",[],[],{"title":241,"pubDate":299,"tags":300,"description":242},["Date","2025-03-04T00:00:00.000Z"],[245,246,247,248,87,19],[],"2014-06-08-setting-up-rails",{"id":302,"data":304,"body":312,"filePath":313,"digest":314,"rendered":315},{"title":305,"description":306,"pubDate":307,"tags":308},"Setting up Rails 4.0.0 with Nginx and Postgres on Digital Ocean","A comprehensive guide to setting up a Ruby on Rails 4.0.0 application with Nginx and PostgreSQL on a Digital Ocean droplet, covering user creation, Ruby installation via RVM, Nginx configuration, and database setup.",["Date","2014-06-08T00:00:00.000Z"],[309,310,311],"Ruby","Rails","Digital Ocean","I decided to tackle learning Ruby on Rails recently. I've heard great stuff about both Ruby and Rails, and have been curious to check them out for a while. I had just the little project, too: a site to help you locate restaurants offering free drink refills. See, in NYC, restaurants won't just give you another coke; oh no, they want to charge you another $2.00 for the privilege. Admittedly, soda is horrendous for you, but once in a while, I get a craving for serious sugar.\n\n## Step 1: Hosting\n\nWhere to host? I've used Heroku in the past, and while it's great for ease of deployment, their database offerings jump straight from free and restrictive to really expensive. A database with any in-memory cache starts at $50/month, which is more than I feel like paying for a toy project. I considered AWS, but my free tier is expiring soon, and on top of that, I prefer Postgres, which RDS doesn't support. Given that this will be a geolocation-based app, I thought Postgres's PostGIS features would be worth trying. A friend recommended \u003Ca href=\"http://www.digitalocean.com\" target=\"_blank\">Digital Ocean\u003C/a>, and the pricing seemed right, so I gave it a shot. A few clicks later, and I had a basic Rails app up.\n\n## Step 2: Setup\n\nFirst issue: The default Ruby is 1.9.1, and Rails is the older 3.2 version. I'm starting afresh, why learn old versions? Plus, it defaults to MySQL. Ugh, hose it. So, I started with a fresh Ubuntu 13.04 x64 install.\n\nFirst, create a user for the webapp:\n\n```bash\nadduser my_rails_app\n```\n\n### Ruby\n\nIt appears that Ruby is out-of-date on Ubuntu 13.04, and gem is considered broken, so the community recommends rvm for installation and configuration. Thankfully, Digital Ocean has a pretty nice guide, that I was able to use with only a few alterations: \u003Ca href=\"http://www.digitalocean.com/community/articles/how-to-install-rails-and-nginx-with-passenger-on-ubuntu\" target=\"_blank\">http://www.digitalocean.com/community/articles/how-to-install-rails-and-nginx-with-passenger-on-ubuntu\u003C/a>.\n\nFirst, download the latest rvm and add the root user to it:\n\n```bash\ncurl -L https://get.rvm.io | bash -s stable\nadduser root rvm\n```\n\nLogout, login to pick up the group changes. Next, use rvm and gem to download the latest Ruby-related packages:\n\n```bash\nrvm get stable\nrvm requirements\nrvm install 2.0.0\nrvm use 2.0.0 --default\nrvm rubygems current\ngem install rails\ngem install passenger&lt;/pre&gt;\naptitude install nodejs # Rails uses Javascript for asset-handling\n```\n\n### Nginx\n\nNow, install a few dev packages necessary to build Nginx:\n\n```bash\naptitude upgrade curl\naptitude install libcurl4-openssl-dev\n```\n\nThe next step would typically be `rvmsudo passenger-install-nginx-module`, which downloads, builds, and configures Nginx, but unfortunately, on the smallest droplet instance (512 MB) it dies for lack of memory. You could temporarily resize the instance, or you can add virtual memory:\n\n```bash\nsudo dd if=/dev/zero of=/swap bs=1M count=1024\nsudo mkswap /swap\nsudo swapon /swap\n```\n\nFinally, install Nginx:\n\n```bash\nrvmsudo passenger-install-nginx-module\n```\n\nUnfortunately, this doesn't install the service scripts to start Nginx on Ubuntu 13.04, so run this:\n\n```bash\n# Download nginx startup script\nwget -O init-deb.sh http://library.linode.com/assets/660-init-deb.sh\n\n# Move the script to the init.d directory and make executable\nmv init-deb.sh /etc/init.d/nginx\nchmod +x /etc/init.d/nginx\n\n# Add nginx to the system startup\n/usr/sbin/update-rc.d -f nginx defaults\n```\n\nIn /opt/nginx/conf/nginx.conf, set the user at the top to the webapp user (only appropriate for a one-app instance.)\n\nChange\n\n```\n#user nobody;\n```\n\nto\n\n```\nuser my_rails_app;\n```\n\nand add a server entry for the webapp:\n\n```\nserver {\n    listen 80;\n    server_name www.example.com;\n    root /var/www/my_rails_app/public; # \u003C--- be sure to point to 'public'!\n    passenger_enabled on;\n}\n```\n\nFinally, restart Nginx with the new configuration:\n\n```bash\n/etc/init.d/nginx start\n```\n\n### Postgres\n\nThe default Postgres for Ubuntu 13.04 is 9.1, which is current enough for my purposes, so we can rely on the lovely apt package system. We also need the pg gem to set up the Rails app with Postgres support, and to add the same user as we did at the start for access.\n\n```bash\naptitude install postgresql libpq-dev\ngem install pg\n\nsu postgres -c 'createuser my_rails_app -dSR'\n/etc/init.d/postgresql start\n```\n\n### Rails\n\nAt last, we're ready to create the Rails app! First, we setup a new directory to store webapps, then we create the app and transfer ownership to the webapp user created at the beginning. Then, we become the user, and use rake to setup the necessary databases.\n\n```bash\nmkdir -p /var/www/\ncd /var/www/\nrails new my_rails_app -d\nchown -R my_rails_app.my_rails_app /var/www/my_rails_app\nsu - my_rails_app\ncd /var/www/my_rails_app\nrake db:create:all\nrake db:migrate\nexit\n```\n\nAt this point, you should be able to run:\n\n```bash\nsu - my_rails_app -c 'cd /var/www/my_rails_app && rails server'\n```\n\nand get the WEBrick server running successfully!","src/content/blog/2014-06-08-setting-up-rails.md","310d56da1f8951d1",{"html":316,"metadata":317},"\u003Cp>I decided to tackle learning Ruby on Rails recently. I’ve heard great stuff about both Ruby and Rails, and have been curious to check them out for a while. I had just the little project, too: a site to help you locate restaurants offering free drink refills. See, in NYC, restaurants won’t just give you another coke; oh no, they want to charge you another $2.00 for the privilege. Admittedly, soda is horrendous for you, but once in a while, I get a craving for serious sugar.\u003C/p>\n\u003Ch2 id=\"step-1-hosting\">Step 1: Hosting\u003C/h2>\n\u003Cp>Where to host? I’ve used Heroku in the past, and while it’s great for ease of deployment, their database offerings jump straight from free and restrictive to really expensive. A database with any in-memory cache starts at $50/month, which is more than I feel like paying for a toy project. I considered AWS, but my free tier is expiring soon, and on top of that, I prefer Postgres, which RDS doesn’t support. Given that this will be a geolocation-based app, I thought Postgres’s PostGIS features would be worth trying. A friend recommended \u003Ca href=\"http://www.digitalocean.com\" target=\"_blank\">Digital Ocean\u003C/a>, and the pricing seemed right, so I gave it a shot. A few clicks later, and I had a basic Rails app up.\u003C/p>\n\u003Ch2 id=\"step-2-setup\">Step 2: Setup\u003C/h2>\n\u003Cp>First issue: The default Ruby is 1.9.1, and Rails is the older 3.2 version. I’m starting afresh, why learn old versions? Plus, it defaults to MySQL. Ugh, hose it. So, I started with a fresh Ubuntu 13.04 x64 install.\u003C/p>\n\u003Cp>First, create a user for the webapp:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">adduser\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> my_rails_app\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"ruby\">Ruby\u003C/h3>\n\u003Cp>It appears that Ruby is out-of-date on Ubuntu 13.04, and gem is considered broken, so the community recommends rvm for installation and configuration. Thankfully, Digital Ocean has a pretty nice guide, that I was able to use with only a few alterations: \u003Ca href=\"http://www.digitalocean.com/community/articles/how-to-install-rails-and-nginx-with-passenger-on-ubuntu\" target=\"_blank\">\u003C/a>\u003Ca href=\"http://www.digitalocean.com/community/articles/how-to-install-rails-and-nginx-with-passenger-on-ubuntu\">http://www.digitalocean.com/community/articles/how-to-install-rails-and-nginx-with-passenger-on-ubuntu\u003C/a>.\u003C/p>\n\u003Cp>First, download the latest rvm and add the root user to it:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">curl\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -L\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> https://get.rvm.io\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\"> bash\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -s\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> stable\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">adduser\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> root\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> rvm\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Logout, login to pick up the group changes. Next, use rvm and gem to download the latest Ruby-related packages:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rvm\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> get\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> stable\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rvm\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> requirements\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rvm\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2.0.0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rvm\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> use\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2.0.0\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --default\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rvm\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> rubygems\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> current\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">gem\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> rails\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">gem\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> passenger\u003C/span>\u003Cspan style=\"color:#E1E4E8\">&#x26;\u003C/span>\u003Cspan style=\"color:#B392F0\">lt\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003Cspan style=\"color:#B392F0\">/pre\u003C/span>\u003Cspan style=\"color:#E1E4E8\">&#x26;\u003C/span>\u003Cspan style=\"color:#B392F0\">gt\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">aptitude\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> nodejs\u003C/span>\u003Cspan style=\"color:#6A737D\"> # Rails uses Javascript for asset-handling\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"nginx\">Nginx\u003C/h3>\n\u003Cp>Now, install a few dev packages necessary to build Nginx:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">aptitude\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> upgrade\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> curl\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">aptitude\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> libcurl4-openssl-dev\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The next step would typically be \u003Ccode>rvmsudo passenger-install-nginx-module\u003C/code>, which downloads, builds, and configures Nginx, but unfortunately, on the smallest droplet instance (512 MB) it dies for lack of memory. You could temporarily resize the instance, or you can add virtual memory:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">sudo\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> dd\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> if=/dev/zero\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> of=/swap\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> bs=1M\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> count=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1024\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">sudo\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> mkswap\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /swap\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">sudo\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> swapon\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /swap\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Finally, install Nginx:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rvmsudo\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> passenger-install-nginx-module\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Unfortunately, this doesn’t install the service scripts to start Nginx on Ubuntu 13.04, so run this:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Download nginx startup script\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">wget\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -O\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> init-deb.sh\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> http://library.linode.com/assets/660-init-deb.sh\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Move the script to the init.d directory and make executable\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">mv\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> init-deb.sh\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /etc/init.d/nginx\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">chmod\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> +x\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /etc/init.d/nginx\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Add nginx to the system startup\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">/usr/sbin/update-rc.d\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -f\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> nginx\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> defaults\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>In /opt/nginx/conf/nginx.conf, set the user at the top to the webapp user (only appropriate for a one-app instance.)\u003C/p>\n\u003Cp>Change\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>#user nobody;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>to\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>user my_rails_app;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>and add a server entry for the webapp:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>server {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>    listen 80;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>    server_name www.example.com;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>    root /var/www/my_rails_app/public; # &#x3C;--- be sure to point to 'public'!\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>    passenger_enabled on;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Finally, restart Nginx with the new configuration:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">/etc/init.d/nginx\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> start\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"postgres\">Postgres\u003C/h3>\n\u003Cp>The default Postgres for Ubuntu 13.04 is 9.1, which is current enough for my purposes, so we can rely on the lovely apt package system. We also need the pg gem to set up the Rails app with Postgres support, and to add the same user as we did at the start for access.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">aptitude\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> postgresql\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> libpq-dev\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">gem\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> pg\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">su\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> postgres\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -c\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'createuser my_rails_app -dSR'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">/etc/init.d/postgresql\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> start\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"rails\">Rails\u003C/h3>\n\u003Cp>At last, we’re ready to create the Rails app! First, we setup a new directory to store webapps, then we create the app and transfer ownership to the webapp user created at the beginning. Then, we become the user, and use rake to setup the necessary databases.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">mkdir\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -p\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /var/www/\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">cd\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /var/www/\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rails\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> new\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> my_rails_app\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -d\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">chown\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -R\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> my_rails_app.my_rails_app\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /var/www/my_rails_app\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">su\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> -\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> my_rails_app\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">cd\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /var/www/my_rails_app\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rake\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> db:create:all\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">rake\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> db:migrate\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">exit\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>At this point, you should be able to run:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">su\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> -\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> my_rails_app\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -c\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'cd /var/www/my_rails_app &#x26;&#x26; rails server'\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>and get the WEBrick server running successfully!\u003C/p>",{"headings":318,"localImagePaths":335,"remoteImagePaths":336,"frontmatter":337,"imagePaths":340},[319,322,325,327,330,333],{"depth":43,"slug":320,"text":321},"step-1-hosting","Step 1: Hosting",{"depth":43,"slug":323,"text":324},"step-2-setup","Step 2: Setup",{"depth":47,"slug":326,"text":309},"ruby",{"depth":47,"slug":328,"text":329},"nginx","Nginx",{"depth":47,"slug":331,"text":332},"postgres","Postgres",{"depth":47,"slug":334,"text":310},"rails",[],[],{"title":305,"pubDate":338,"tags":339,"description":306},["Date","2014-06-08T00:00:00.000Z"],[309,310,311],[],"2022-06-22-tidd",{"id":341,"data":343,"body":353,"filePath":354,"digest":355,"rendered":356},{"title":344,"description":345,"pubDate":346,"tags":347},"Test-induced design damage in Clojure","An exploration of test-induced design damage (TIDD) in Clojure, examining how code modifications made solely for testing can lead to unnecessary complexity, with comparisons of various approaches including with-redefs, binding, protocols, and the dynamic-redef library.",["Date","2022-06-22T00:00:00.000Z"],[18,348,349,350,351,352],"TIDD","TDD","test","redef","dynamic redef","Test-Induced Design Damage (TIDD) is not a new concept. DHH of Rails [wrote about TIDD](https://dhh.dk/2014/test-induced-design-damage.html) back in 2014. This is not even a new concept for Clojure, as Eric Normand [wrote a newsletter about it](https://ericnormand.me/issues/purelyfunctional-tv-newsletter-325-tip-don-t-use-a-protocol-to-make-testing-easier) in the past. Unfortunately, Normand's post didn't have the impact I'd hoped for, nor did it go into enough detail for me, so I'm going to try and give examples that will help people understand the issue and the trade-offs a bit better.\n\n## What is test-induced design damage?\n\nI suggest you read DHH's post above, but in short, it's altering code to better support tests at the expense of other aspects of the system. Communities like Test-Driven Development (TDD) have taken it as _a priori_ gospel that better testing is a primary goal, while downplaying the consequences. As they say, software developers know the value of everything and the cost of nothing.\n\nFor example, extracting hidden/private/closed-over code so it can be mocked for testing also carries detriments like requiring names (it usually can't be anonymous any more), cluttering docs, expanding argument lists (since support objects must be passed-in or injected), potential misuse (if users can now directly access/create/use an object when they shouldn't), and indirection overhead (both mental and code-based).\n\n**To be clear: alterations that support testing may have other benefits that justify their use, but this needs to be evaluated on a case-by-case basis, rather than assuming improved testing is a sufficient justification _itself_.** \n\nIt's even possible that complicating your code to support testing actually **increases the number of bugs**, despite more testing. This is because, all other things being equal, larger codebases have more bugs[^large-bugs].\n\nThe rest of this post will look at changes made *solely* for testing. I'll show you some examples of TIDD, in order of increasing complexity. \n\n## Examples\n\nImagine you have some function that takes too long to use in local tests. Maybe it makes a network call that takes a while.\n\nLet's say you made a plain function:\n\n```clojure\n(defn my-fn \n  []\n  (call-prod-endpoint))\n\n;; usage\n(my-fn)\n```\n\nYou decide you need to mock it for testing, so what are your options?\n\n### Redefining via `with-redefs` or `with-redefs-fn`\n\nUsing `with-redefs`, you can temporarily replace the root definition for testing without touching the original code at all (`alter-var-root` can work, too, though it's more cumbersome to use). This sounds like the perfect way to leave non-testing code clean, right? Eric Normand suggested this in [his original newsletter.](https://ericnormand.me/issues/purelyfunctional-tv-newsletter-325-tip-don-t-use-a-protocol-to-make-testing-easier)\n\n```clojure\n(defn my-fn \n  []\n  (call-prod-endpoint))\n\n;; test\n(deftest redef-ing-my-fn\n  (with-redefs [my-fn #(call-mock-endpoint)]\n    (is (= (my-fn) some-expected-result))))\n```\n\nUnfortunately, `with-redefs` requires a lot of care with multi-threaded / lazy code, since the var root definition is changed for _all_ threads for a limited time. Code in other threads that run after the `with-redefs` ends can easily use an unintended value. Tim Baldridge wrote a long post on [how vars work under the hood and why redefs can be tricky](https://cognitect.com/blog/2016/9/15/works-on-my-machine-understanding-var-bindings-and-roots), and it's worth reading before using functions like `binding/with-redefs` in any context.\n\nYou _could_ safely use `with-redefs` if you can guarantee all of the following:\n\n1. Don't run multiple tests simultaneously - slower unit testing is the price\n2. Don't rely on background threads - these are fragile anyway, and create timing concerns even if you don't redef anything\n3. Wrap the entire body of the test in `with-redefs` - you don't need to worry about lazy evaluation happening after `with-redefs` ends if you've already forced the values you need\n4. Ensure you always join with other threads before exiting the `with-redefs` if those threads do anything an assertion relies on - this may require code distortion itself\n\nThese are a lot of constraints. #1 is undesirable if easy, but ensuring #2 and #4 may range from annoying to infeasible without altering our main code, which violates the goal of avoiding TIDD.\n\n(There's also a long discussion about `with-redefs` in the [Reddit comments on Eric Normand's original newsletter](https://www.reddit.com/r/Clojure/comments/ble9k4/dont_use_a_protocol_to_make_testing_easier/). Unfortunately, Eric's example involved a database connection, which inherently has state and was thus a stronger candidate for protocols/components, and many people latched onto that aspect instead of considering the bigger picture.)\n\n### Rebinding via `binding` or `with-bindings`\n\nThis is similar to the above, and works, but it requires you declare `my-fn` as `^:dynamic`. \n\n```clojure\n(defn ^:dynamic my-fn \n  []\n  (call-prod-endpoint))\n\n;; test usage\n(deftest rebinding-my-fn\n  (binding [my-fn #(call-mock-endpoint)]\n    (is (= (my-fn) some-expected-result))))  \n```\nOn the upside, it only changes the local thread and its children's definition, so tests can run in parallel. Care must still be taken with background threads, but you should avoid those in tests anyway. For all other started threads, they'll carry the binding frame with them, even if the top thread ended, so it's much safer for multi-threaded code.\n\nHowever, this is still a slight alteration of the code. Declaring it as `^:dynamic` means it's slightly slower to execute in production code. Worse, it sends a false signal to users that they may need/want to rebind it. Plus, it suffers a variant of the expression problem, since you cannot mark outside vars as `^:dynamic` without forking the code. (One can argue you shouldn't test outside code, but creating wrapper fns just to mock is TIDD again.) Still, this is almost the ideal solution, if not for `^:dynamic`.[^dark-arts]\n\n### Branch inside the function on a testing flag\n\nThis might be an option if you already use feature flags heavily. For testing, it would look something like: \n\n```clojure\n(defn my-fn \n  []\n  (if-not global.flags/is-testing?\n    (call-prod-endpoint))\n    (call-mock-endpoint))\n```\n\nThen you need to set `global.flags/is-testing?` only when testing. This keeps the function signature clean, but clutters the global namespace, complicates the function body, makes multiple mock behaviors difficult, and adds branching overhead. \n\nYou could also use compile-time constants or macros to make this pattern more efficient, but it would still be less flexible and cluttered.\n\n### Multimethods\n\nWhat about polymorphism? You could make `my-fn` polymorphic with multimethods by dispatching based on whether you're running normally or for testing:\n\n```clojure\n\n(defmulti my-fn (fn [type] type))\n\n(defmethod my-fn :normal [_]\n  (call-prod-endpoint))\n\n(defmethod my-fn :test [_]\n  (call-mock-endpoint))\n\n;; usage\n(my-fn :normal)\n\n;; test usage\n(deftest polymorphic-multimethod-test\n  (is (= (my-fn :test) some-expected-result)))\n\n```\n\nThe problem is you now have more code, and you have to weave the right dispatch value into **all** calls to `my-fn` (and possibly their parents), which alters the param signatures. You could set the dispatch value as a global var, but that has many of the same problems as internal branching does.\n\nWhich leaves protocols...\n\n## Protocols\n\nThe pattern I've seen the most in real Clojure code, and unfortunately, the **most complicated** option, is to replace plain functions with protocols and records.\n\n```clojure\n(defprotocol MyProtocol\n  (my-fn [_]))\n\n(defrecord MyFunctionner []\n  MyProtocol\n  (my-fn [_]\n    (call-prod-endpoint)))\n\n(defrecord MyTestFunctionner []\n  MyProtocol\n  (my-fn [_]\n    (call-mock-endpoint)))\n\n;; non-default constructors are commonly added\n(defn my-functionner []\n  (->MyFunctionner))\n\n(defn my-test-functionner []\n  (->MyTestFunctionner))\n\n;; usage \n\n(let [my-fn-er (my-functionner)]\n  (my-fn my-fn-er))\n\n\n;; add component deps for bonus points\n(def system\n ...\n :my-functioner (my-functionner)\n :something-else (component/using\n                   (something-else)\n                   [:my-functionner ...]))\n```\n\nProtocols have the inherent problem of requiring state, since they can only be used with an object. Even if the type/record defines no state internally, _lifecycle state itself must be taken into consideration_. Unlike a function or multimethod, which is effectively available once its namespace is required, protocol functions cannot be used before an object is created or after it's destroyed. Plus, the object must be passed around everywhere it's used, cluttering up argument lists and adding to naming overhead everywhere.\n\nFor bonus complexity, non-default constructors are extremely common additions, and once people have a type/record with a lifecycle, they add it to their initialization system, so they end up writing a bunch of extra Component/Integrant/etc code to support it, too.\n\n**Is all this worth it?** How many protocols have you seen that exist _just_ to support testing and nothing else?\n\n## Solution: dynamic redef\n\nThe solution I've settled on is [one created by Mourjo Sen](https://medium.com/helpshift-engineering/a-study-in-parallelising-tests-b5253817beae) and I think it deserves to be more widely known. It's encapsulated in a mini-library called [dynamic-redef](https://clojars.org/me.mourjo/dynamic-redef).\n\nThe basic idea is to mimic the propagated thread-local behavior of `binding` without having to declare anything `^:dynamic` or mess with our main code. It uses `alter-var-root` to permanently replace the root definition of a function with one that looks up its current definition in a `^:dynamic` map but falls back to the original definition if no overrides are found. Then \"dynamically redefining\" a function involves adding a new binding frame under the hood with updated fn definitions for the dynamic function lookup map.\n\nHere's his original gist of the technique: \u003Cscript src=\"https://gist.github.com/mourjo/387e78fcb62ab67392115c2ed616c9ab.js\">\u003C/script>\n\nAdvantages:\n\n1. Allows you to leave your main code _completely_ unaltered\n2. Incurs no performance penalty in production code\n3. Replaces definitions in a more thread-safe manner than raw `with-redefs`\n\nDisadvantages:\n\n1. Does not play well with background threads (though you should avoid those in tests when possible)\n2. Like `binding`, does not work with plain Java threading, which doesn't use Clojure thread frames\n\n### Summary\n\nThis is not meant to eliminate testing-specific protocols/records, but to offer an option that's more suitable in some use cases. My personal \"middle way\" of testing is, examine the thing to be mocked and determine if it has inherent state. If so, it's probably a better fit for protocols. But if not, don't complicate your code just to test it. Give dynamic redef a try. It may be unfamiliar, but it's simpler than the alternatives when it fits.\n\n\n###### Footnotes\n\n[^large-bugs]: Code Complete has [some industry-generated estimates on bugs/LOC](https://amartester.blogspot.com/2007/04/bugs-per-lines-of-code.html), but the much-discussed study, [A Large-Scale Study of Programming Languages and Code Quality in Github](https://cacm.acm.org/magazines/2017/10/221326-a-large-scale-study-of-programming-languages-and-code-quality-in-github/fulltext), actually computed the overall effect of code size (independent of language) as a control variable. If you look at the discussion of the control variables in Table 6, \"...they are all positive and significant\". All else being equal, less code means fewer bugs. \n\n[^dark-arts]: Technically, you don't have to declare a var `^:dynamic` to use `binding` on it. There's an undocumented `.setDynamic` method on vars, but to use this dark art successfully, you'd have to invoke it before the compiler gets to any call sites with the var. Otherwise, it'll compile a call to the root definition, and never check for binding frames. I've seen some code that claims to do this reliably via macros, but it doesn't seem to work for me.","src/content/blog/2022-06-22-tidd.md","005ad85fb706e1dd",{"html":357,"metadata":358},"\u003Cp>Test-Induced Design Damage (TIDD) is not a new concept. DHH of Rails \u003Ca href=\"https://dhh.dk/2014/test-induced-design-damage.html\">wrote about TIDD\u003C/a> back in 2014. This is not even a new concept for Clojure, as Eric Normand \u003Ca href=\"https://ericnormand.me/issues/purelyfunctional-tv-newsletter-325-tip-don-t-use-a-protocol-to-make-testing-easier\">wrote a newsletter about it\u003C/a> in the past. Unfortunately, Normand’s post didn’t have the impact I’d hoped for, nor did it go into enough detail for me, so I’m going to try and give examples that will help people understand the issue and the trade-offs a bit better.\u003C/p>\n\u003Ch2 id=\"what-is-test-induced-design-damage\">What is test-induced design damage?\u003C/h2>\n\u003Cp>I suggest you read DHH’s post above, but in short, it’s altering code to better support tests at the expense of other aspects of the system. Communities like Test-Driven Development (TDD) have taken it as \u003Cem>a priori\u003C/em> gospel that better testing is a primary goal, while downplaying the consequences. As they say, software developers know the value of everything and the cost of nothing.\u003C/p>\n\u003Cp>For example, extracting hidden/private/closed-over code so it can be mocked for testing also carries detriments like requiring names (it usually can’t be anonymous any more), cluttering docs, expanding argument lists (since support objects must be passed-in or injected), potential misuse (if users can now directly access/create/use an object when they shouldn’t), and indirection overhead (both mental and code-based).\u003C/p>\n\u003Cp>\u003Cstrong>To be clear: alterations that support testing may have other benefits that justify their use, but this needs to be evaluated on a case-by-case basis, rather than assuming improved testing is a sufficient justification \u003Cem>itself\u003C/em>.\u003C/strong>\u003C/p>\n\u003Cp>It’s even possible that complicating your code to support testing actually \u003Cstrong>increases the number of bugs\u003C/strong>, despite more testing. This is because, all other things being equal, larger codebases have more bugs\u003Csup>\u003Ca href=\"#user-content-fn-large-bugs\" id=\"user-content-fnref-large-bugs\" data-footnote-ref=\"\" aria-describedby=\"footnote-label\">1\u003C/a>\u003C/sup>.\u003C/p>\n\u003Cp>The rest of this post will look at changes made \u003Cem>solely\u003C/em> for testing. I’ll show you some examples of TIDD, in order of increasing complexity.\u003C/p>\n\u003Ch2 id=\"examples\">Examples\u003C/h2>\n\u003Cp>Imagine you have some function that takes too long to use in local tests. Maybe it makes a network call that takes a while.\u003C/p>\n\u003Cp>Let’s say you made a plain function:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"clojure\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defn\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-prod-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; usage\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>You decide you need to mock it for testing, so what are your options?\u003C/p>\n\u003Ch3 id=\"redefining-via-with-redefs-or-with-redefs-fn\">Redefining via \u003Ccode>with-redefs\u003C/code> or \u003Ccode>with-redefs-fn\u003C/code>\u003C/h3>\n\u003Cp>Using \u003Ccode>with-redefs\u003C/code>, you can temporarily replace the root definition for testing without touching the original code at all (\u003Ccode>alter-var-root\u003C/code> can work, too, though it’s more cumbersome to use). This sounds like the perfect way to leave non-testing code clean, right? Eric Normand suggested this in \u003Ca href=\"https://ericnormand.me/issues/purelyfunctional-tv-newsletter-325-tip-don-t-use-a-protocol-to-make-testing-easier\">his original newsletter.\u003C/a>\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"clojure\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defn\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-prod-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; test\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">deftest\u003C/span>\u003Cspan style=\"color:#B392F0\"> redef-ing-my-fn\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">with-redefs\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [my-fn #(\u003C/span>\u003Cspan style=\"color:#B392F0\">call-mock-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    (\u003C/span>\u003Cspan style=\"color:#B392F0\">is\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) some-expected-result))))\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Unfortunately, \u003Ccode>with-redefs\u003C/code> requires a lot of care with multi-threaded / lazy code, since the var root definition is changed for \u003Cem>all\u003C/em> threads for a limited time. Code in other threads that run after the \u003Ccode>with-redefs\u003C/code> ends can easily use an unintended value. Tim Baldridge wrote a long post on \u003Ca href=\"https://cognitect.com/blog/2016/9/15/works-on-my-machine-understanding-var-bindings-and-roots\">how vars work under the hood and why redefs can be tricky\u003C/a>, and it’s worth reading before using functions like \u003Ccode>binding/with-redefs\u003C/code> in any context.\u003C/p>\n\u003Cp>You \u003Cem>could\u003C/em> safely use \u003Ccode>with-redefs\u003C/code> if you can guarantee all of the following:\u003C/p>\n\u003Col>\n\u003Cli>Don’t run multiple tests simultaneously - slower unit testing is the price\u003C/li>\n\u003Cli>Don’t rely on background threads - these are fragile anyway, and create timing concerns even if you don’t redef anything\u003C/li>\n\u003Cli>Wrap the entire body of the test in \u003Ccode>with-redefs\u003C/code> - you don’t need to worry about lazy evaluation happening after \u003Ccode>with-redefs\u003C/code> ends if you’ve already forced the values you need\u003C/li>\n\u003Cli>Ensure you always join with other threads before exiting the \u003Ccode>with-redefs\u003C/code> if those threads do anything an assertion relies on - this may require code distortion itself\u003C/li>\n\u003C/ol>\n\u003Cp>These are a lot of constraints. #1 is undesirable if easy, but ensuring #2 and #4 may range from annoying to infeasible without altering our main code, which violates the goal of avoiding TIDD.\u003C/p>\n\u003Cp>(There’s also a long discussion about \u003Ccode>with-redefs\u003C/code> in the \u003Ca href=\"https://www.reddit.com/r/Clojure/comments/ble9k4/dont_use_a_protocol_to_make_testing_easier/\">Reddit comments on Eric Normand’s original newsletter\u003C/a>. Unfortunately, Eric’s example involved a database connection, which inherently has state and was thus a stronger candidate for protocols/components, and many people latched onto that aspect instead of considering the bigger picture.)\u003C/p>\n\u003Ch3 id=\"rebinding-via-binding-or-with-bindings\">Rebinding via \u003Ccode>binding\u003C/code> or \u003Ccode>with-bindings\u003C/code>\u003C/h3>\n\u003Cp>This is similar to the above, and works, but it requires you declare \u003Ccode>my-fn\u003C/code> as \u003Ccode>^:dynamic\u003C/code>.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"clojure\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ^:dynamic \u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-prod-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; test usage\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">deftest\u003C/span>\u003Cspan style=\"color:#B392F0\"> rebinding-my-fn\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#F97583\">binding\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [my-fn #(\u003C/span>\u003Cspan style=\"color:#B392F0\">call-mock-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    (\u003C/span>\u003Cspan style=\"color:#B392F0\">is\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) some-expected-result))))  \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>On the upside, it only changes the local thread and its children’s definition, so tests can run in parallel. Care must still be taken with background threads, but you should avoid those in tests anyway. For all other started threads, they’ll carry the binding frame with them, even if the top thread ended, so it’s much safer for multi-threaded code.\u003C/p>\n\u003Cp>However, this is still a slight alteration of the code. Declaring it as \u003Ccode>^:dynamic\u003C/code> means it’s slightly slower to execute in production code. Worse, it sends a false signal to users that they may need/want to rebind it. Plus, it suffers a variant of the expression problem, since you cannot mark outside vars as \u003Ccode>^:dynamic\u003C/code> without forking the code. (One can argue you shouldn’t test outside code, but creating wrapper fns just to mock is TIDD again.) Still, this is almost the ideal solution, if not for \u003Ccode>^:dynamic\u003C/code>.\u003Csup>\u003Ca href=\"#user-content-fn-dark-arts\" id=\"user-content-fnref-dark-arts\" data-footnote-ref=\"\" aria-describedby=\"footnote-label\">2\u003C/a>\u003C/sup>\u003C/p>\n\u003Ch3 id=\"branch-inside-the-function-on-a-testing-flag\">Branch inside the function on a testing flag\u003C/h3>\n\u003Cp>This might be an option if you already use feature flags heavily. For testing, it would look something like:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"clojure\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defn\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#F97583\">if-not\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> global.flags/is-testing?\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-prod-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-mock-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Then you need to set \u003Ccode>global.flags/is-testing?\u003C/code> only when testing. This keeps the function signature clean, but clutters the global namespace, complicates the function body, makes multiple mock behaviors difficult, and adds branching overhead.\u003C/p>\n\u003Cp>You could also use compile-time constants or macros to make this pattern more efficient, but it would still be less flexible and cluttered.\u003C/p>\n\u003Ch3 id=\"multimethods\">Multimethods\u003C/h3>\n\u003Cp>What about polymorphism? You could make \u003Ccode>my-fn\u003C/code> polymorphic with multimethods by dispatching based on whether you’re running normally or for testing:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"clojure\">\u003Ccode>\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defmulti\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [type] type))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defmethod\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-fn\u003C/span>\u003Cspan style=\"color:#79B8FF\"> :normal\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [_]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-prod-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defmethod\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-fn\u003C/span>\u003Cspan style=\"color:#79B8FF\"> :test\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [_]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-mock-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; usage\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#79B8FF\"> :normal\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; test usage\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">deftest\u003C/span>\u003Cspan style=\"color:#B392F0\"> polymorphic-multimethod-test\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">is\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#79B8FF\"> :test\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) some-expected-result)))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The problem is you now have more code, and you have to weave the right dispatch value into \u003Cstrong>all\u003C/strong> calls to \u003Ccode>my-fn\u003C/code> (and possibly their parents), which alters the param signatures. You could set the dispatch value as a global var, but that has many of the same problems as internal branching does.\u003C/p>\n\u003Cp>Which leaves protocols…\u003C/p>\n\u003Ch2 id=\"protocols\">Protocols\u003C/h2>\n\u003Cp>The pattern I’ve seen the most in real Clojure code, and unfortunately, the \u003Cstrong>most complicated\u003C/strong> option, is to replace plain functions with protocols and records.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"clojure\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defprotocol\u003C/span>\u003Cspan style=\"color:#B392F0\"> MyProtocol\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [_]))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defrecord\u003C/span>\u003Cspan style=\"color:#B392F0\"> MyFunctionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  MyProtocol\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [_]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-prod-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defrecord\u003C/span>\u003Cspan style=\"color:#B392F0\"> MyTestFunctionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  MyProtocol\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [_]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    (\u003C/span>\u003Cspan style=\"color:#B392F0\">call-mock-endpoint\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; non-default constructors are commonly added\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defn\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-functionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">->MyFunctionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">defn\u003C/span>\u003Cspan style=\"color:#B392F0\"> my-test-functionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">->MyTestFunctionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; usage \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">let\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [my-fn-er (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-functionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-fn\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> my-fn-er))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">;; add component deps for bonus points\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> system\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\"> ...\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\"> :my-functioner\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">my-functionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\"> :something-else\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#B392F0\">component/using\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                   (\u003C/span>\u003Cspan style=\"color:#B392F0\">something-else\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                   [\u003C/span>\u003Cspan style=\"color:#79B8FF\">:my-functionner\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ...]))\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Protocols have the inherent problem of requiring state, since they can only be used with an object. Even if the type/record defines no state internally, \u003Cem>lifecycle state itself must be taken into consideration\u003C/em>. Unlike a function or multimethod, which is effectively available once its namespace is required, protocol functions cannot be used before an object is created or after it’s destroyed. Plus, the object must be passed around everywhere it’s used, cluttering up argument lists and adding to naming overhead everywhere.\u003C/p>\n\u003Cp>For bonus complexity, non-default constructors are extremely common additions, and once people have a type/record with a lifecycle, they add it to their initialization system, so they end up writing a bunch of extra Component/Integrant/etc code to support it, too.\u003C/p>\n\u003Cp>\u003Cstrong>Is all this worth it?\u003C/strong> How many protocols have you seen that exist \u003Cem>just\u003C/em> to support testing and nothing else?\u003C/p>\n\u003Ch2 id=\"solution-dynamic-redef\">Solution: dynamic redef\u003C/h2>\n\u003Cp>The solution I’ve settled on is \u003Ca href=\"https://medium.com/helpshift-engineering/a-study-in-parallelising-tests-b5253817beae\">one created by Mourjo Sen\u003C/a> and I think it deserves to be more widely known. It’s encapsulated in a mini-library called \u003Ca href=\"https://clojars.org/me.mourjo/dynamic-redef\">dynamic-redef\u003C/a>.\u003C/p>\n\u003Cp>The basic idea is to mimic the propagated thread-local behavior of \u003Ccode>binding\u003C/code> without having to declare anything \u003Ccode>^:dynamic\u003C/code> or mess with our main code. It uses \u003Ccode>alter-var-root\u003C/code> to permanently replace the root definition of a function with one that looks up its current definition in a \u003Ccode>^:dynamic\u003C/code> map but falls back to the original definition if no overrides are found. Then “dynamically redefining” a function involves adding a new binding frame under the hood with updated fn definitions for the dynamic function lookup map.\u003C/p>\n\u003Cp>Here’s his original gist of the technique: \u003Cscript src=\"https://gist.github.com/mourjo/387e78fcb62ab67392115c2ed616c9ab.js\">\u003C/script>\u003C/p>\n\u003Cp>Advantages:\u003C/p>\n\u003Col>\n\u003Cli>Allows you to leave your main code \u003Cem>completely\u003C/em> unaltered\u003C/li>\n\u003Cli>Incurs no performance penalty in production code\u003C/li>\n\u003Cli>Replaces definitions in a more thread-safe manner than raw \u003Ccode>with-redefs\u003C/code>\u003C/li>\n\u003C/ol>\n\u003Cp>Disadvantages:\u003C/p>\n\u003Col>\n\u003Cli>Does not play well with background threads (though you should avoid those in tests when possible)\u003C/li>\n\u003Cli>Like \u003Ccode>binding\u003C/code>, does not work with plain Java threading, which doesn’t use Clojure thread frames\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"summary\">Summary\u003C/h3>\n\u003Cp>This is not meant to eliminate testing-specific protocols/records, but to offer an option that’s more suitable in some use cases. My personal “middle way” of testing is, examine the thing to be mocked and determine if it has inherent state. If so, it’s probably a better fit for protocols. But if not, don’t complicate your code just to test it. Give dynamic redef a try. It may be unfamiliar, but it’s simpler than the alternatives when it fits.\u003C/p>\n\u003Ch6 id=\"footnotes\">Footnotes\u003C/h6>\n\u003Csection data-footnotes=\"\" class=\"footnotes\">\u003Ch2 class=\"sr-only\" id=\"footnote-label\">Footnotes\u003C/h2>\n\u003Col>\n\u003Cli id=\"user-content-fn-large-bugs\">\n\u003Cp>Code Complete has \u003Ca href=\"https://amartester.blogspot.com/2007/04/bugs-per-lines-of-code.html\">some industry-generated estimates on bugs/LOC\u003C/a>, but the much-discussed study, \u003Ca href=\"https://cacm.acm.org/magazines/2017/10/221326-a-large-scale-study-of-programming-languages-and-code-quality-in-github/fulltext\">A Large-Scale Study of Programming Languages and Code Quality in Github\u003C/a>, actually computed the overall effect of code size (independent of language) as a control variable. If you look at the discussion of the control variables in Table 6, “…they are all positive and significant”. All else being equal, less code means fewer bugs. \u003Ca href=\"#user-content-fnref-large-bugs\" data-footnote-backref=\"\" aria-label=\"Back to reference 1\" class=\"data-footnote-backref\">↩\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli id=\"user-content-fn-dark-arts\">\n\u003Cp>Technically, you don’t have to declare a var \u003Ccode>^:dynamic\u003C/code> to use \u003Ccode>binding\u003C/code> on it. There’s an undocumented \u003Ccode>.setDynamic\u003C/code> method on vars, but to use this dark art successfully, you’d have to invoke it before the compiler gets to any call sites with the var. Otherwise, it’ll compile a call to the root definition, and never check for binding frames. I’ve seen some code that claims to do this reliably via macros, but it doesn’t seem to work for me. \u003Ca href=\"#user-content-fnref-dark-arts\" data-footnote-backref=\"\" aria-label=\"Back to reference 2\" class=\"data-footnote-backref\">↩\u003C/a>\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003C/section>",{"headings":359,"localImagePaths":393,"remoteImagePaths":394,"frontmatter":395,"imagePaths":398},[360,363,366,369,372,375,378,381,384,387,391],{"depth":43,"slug":361,"text":362},"what-is-test-induced-design-damage","What is test-induced design damage?",{"depth":43,"slug":364,"text":365},"examples","Examples",{"depth":47,"slug":367,"text":368},"redefining-via-with-redefs-or-with-redefs-fn","Redefining via with-redefs or with-redefs-fn",{"depth":47,"slug":370,"text":371},"rebinding-via-binding-or-with-bindings","Rebinding via binding or with-bindings",{"depth":47,"slug":373,"text":374},"branch-inside-the-function-on-a-testing-flag","Branch inside the function on a testing flag",{"depth":47,"slug":376,"text":377},"multimethods","Multimethods",{"depth":43,"slug":379,"text":380},"protocols","Protocols",{"depth":43,"slug":382,"text":383},"solution-dynamic-redef","Solution: dynamic redef",{"depth":47,"slug":385,"text":386},"summary","Summary",{"depth":388,"slug":389,"text":390},6,"footnotes","Footnotes",{"depth":43,"slug":392,"text":390},"footnote-label",[],[],{"title":344,"pubDate":396,"tags":397,"description":345},["Date","2022-06-22T00:00:00.000Z"],[18,348,349,350,351,352],[],"2017-03-28-setting-up-algo",{"id":399,"data":401,"body":409,"filePath":410,"digest":411,"rendered":412},{"title":402,"description":403,"pubDate":404,"tags":405},"Using Algo to setup the strongSwan VPN","A practical guide to setting up a personal VPN using Algo and strongSwan on an existing Ubuntu server, including solutions to common issues with SSH connections, firewall configurations, and root access requirements.",["Date","2017-03-28T00:00:00.000Z"],[406,407,408],"VPN","Algo","Strongswan","Recently, it was in the news that [Congress declined to prevent ISPs from requiring consumer opt-in before ~~spying on you~~ selling your browsing history to advertisers](https://www.washingtonpost.com/news/the-switch/wp/2017/03/28/the-house-just-voted-to-wipe-out-the-fccs-landmark-internet-privacy-protections/). Of course, the rule they're trying to kill hasn't even been applied yet, so ISPs do this currently *anyway*. Browser extensions like [HTTPS Everywhere](https://www.eff.org/https-everywhere) help, since with HTTPS, ISPs can only see what domains you connect to, but even that's a bit much.\n\nThe natural solution is a VPN. Tunnel all your traffic through another server, and there's nothing for an ISP or malicious coffee shop to spy on. There's no shortage of commercial VPNs offering their services, but all of them require that you trust them over your ISP. This might be reasonable, or it might not be. Personally, I already *had* a server set up for this website, so I decided to add a VPN to it.\n\nI used [Algo](https://blog.trailofbits.com/2016/12/12/meet-algo-the-vpn-that-works/), since I'm not a strongSwan configuration expert, and it promised to get me up and running quickly. By default, it creates a brand-new server, which is simpler, but it supports existing servers, too, albeit with less support. What follows are the steps I took to make it work on an existing Ubuntu 16.10 server on Vultr. Hopefully this helps.\n\n## Algo\n\nAlgo runs as a shell script which collects some info and then runs a series of Ansible playbooks. To get started, just run:\n\n```shell\n$ ./algo\n\n  What provider would you like to use?\n    1. DigitalOcean\n    2. Amazon EC2\n    3. Google Compute Engine\n    4. Microsoft Azure\n    5. Install to existing Ubuntu server\n\nEnter the number of your desired provider\n: 5\n\n...\n```\nSelect your options, and if all goes well, great! You can stop reading this post. (See [Algo's docs](https://github.com/trailofbits/algo) for more.)\n\n## Problems I Encountered\n\n### Root login is disabled\n\nI disabled root login, and do everything through my personal user with `sudo`. However, Algo really expects to run as root. To fix this, open up the `ansible.cfg` file, and add the privilege escalation section:\n\n```ini\n[privilege_escalation]\nbecome = True\nbecome_user = root\nbecome_ask_pass = True\n```\n\nIf you allow root login, there should be no need for the above step.\n\n### SSH connection errors\n\nI added the server's SSH key to ssh-agent, but for some inexplicable reason, Algo couldn't connect. After trying half a dozen possibilities, the culprit turned out to be the SSH option, `-o IdentitiesOnly=yes`. This forces SSH to use only identities in the main `~/.ssh/config` file or passed-in on the command-line. But since I created a special keypair just for Vultr, it can't use it. The solution is to remove `-o IdentitiesOnly=yes` from `ssh_args` in `ansible.cfg`.\n\n### Firewall issues\n\nAfter Algo completed, my monitoring service immediately blew up my phone to tell me the website was down. Turns out, it disabled UFW, and with it, my web server firewall ports. Luckily the fix for that was just a simple re-enable on the server:\n\n```shell\nsudo ufw enable\n```\n(Note: UFW won't show the VPN's firewall rules, but iptables will.)\n\nFinally, for Vultr, they (wisely) set up an external firewall available for instances. This is generally a good idea, but trying to match it to the rules necessary for the VPN is a lot of work, so the final thing to do is to disable the firewall from the Vultr control panel. **Be sure your firewall is set up correctly on the server first.**","src/content/blog/2017-03-28-setting-up-algo.md","cd12596502f050ed",{"html":413,"metadata":414},"\u003Cp>Recently, it was in the news that \u003Ca href=\"https://www.washingtonpost.com/news/the-switch/wp/2017/03/28/the-house-just-voted-to-wipe-out-the-fccs-landmark-internet-privacy-protections/\">Congress declined to prevent ISPs from requiring consumer opt-in before \u003Cdel>spying on you\u003C/del> selling your browsing history to advertisers\u003C/a>. Of course, the rule they’re trying to kill hasn’t even been applied yet, so ISPs do this currently \u003Cem>anyway\u003C/em>. Browser extensions like \u003Ca href=\"https://www.eff.org/https-everywhere\">HTTPS Everywhere\u003C/a> help, since with HTTPS, ISPs can only see what domains you connect to, but even that’s a bit much.\u003C/p>\n\u003Cp>The natural solution is a VPN. Tunnel all your traffic through another server, and there’s nothing for an ISP or malicious coffee shop to spy on. There’s no shortage of commercial VPNs offering their services, but all of them require that you trust them over your ISP. This might be reasonable, or it might not be. Personally, I already \u003Cem>had\u003C/em> a server set up for this website, so I decided to add a VPN to it.\u003C/p>\n\u003Cp>I used \u003Ca href=\"https://blog.trailofbits.com/2016/12/12/meet-algo-the-vpn-that-works/\">Algo\u003C/a>, since I’m not a strongSwan configuration expert, and it promised to get me up and running quickly. By default, it creates a brand-new server, which is simpler, but it supports existing servers, too, albeit with less support. What follows are the steps I took to make it work on an existing Ubuntu 16.10 server on Vultr. Hopefully this helps.\u003C/p>\n\u003Ch2 id=\"algo\">Algo\u003C/h2>\n\u003Cp>Algo runs as a shell script which collects some info and then runs a series of Ansible playbooks. To get started, just run:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"shell\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">$\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ./algo\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  What\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> provider\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> would\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> you\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> like\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> to\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> use?\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">    1.\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> DigitalOcean\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">    2.\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Amazon\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> EC2\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">    3.\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Google\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Compute\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Engine\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">    4.\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Microsoft\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Azure\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">    5.\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> to\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> existing\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Ubuntu\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> server\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">Enter\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> the\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> number\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> of\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> your\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> desired\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> provider\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">:\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 5\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Select your options, and if all goes well, great! You can stop reading this post. (See \u003Ca href=\"https://github.com/trailofbits/algo\">Algo’s docs\u003C/a> for more.)\u003C/p>\n\u003Ch2 id=\"problems-i-encountered\">Problems I Encountered\u003C/h2>\n\u003Ch3 id=\"root-login-is-disabled\">Root login is disabled\u003C/h3>\n\u003Cp>I disabled root login, and do everything through my personal user with \u003Ccode>sudo\u003C/code>. However, Algo really expects to run as root. To fix this, open up the \u003Ccode>ansible.cfg\u003C/code> file, and add the privilege escalation section:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"ini\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">[privilege_escalation]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">become\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> = True\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">become_user\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> = root\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">become_ask_pass\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> = True\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>If you allow root login, there should be no need for the above step.\u003C/p>\n\u003Ch3 id=\"ssh-connection-errors\">SSH connection errors\u003C/h3>\n\u003Cp>I added the server’s SSH key to ssh-agent, but for some inexplicable reason, Algo couldn’t connect. After trying half a dozen possibilities, the culprit turned out to be the SSH option, \u003Ccode>-o IdentitiesOnly=yes\u003C/code>. This forces SSH to use only identities in the main \u003Ccode>~/.ssh/config\u003C/code> file or passed-in on the command-line. But since I created a special keypair just for Vultr, it can’t use it. The solution is to remove \u003Ccode>-o IdentitiesOnly=yes\u003C/code> from \u003Ccode>ssh_args\u003C/code> in \u003Ccode>ansible.cfg\u003C/code>.\u003C/p>\n\u003Ch3 id=\"firewall-issues\">Firewall issues\u003C/h3>\n\u003Cp>After Algo completed, my monitoring service immediately blew up my phone to tell me the website was down. Turns out, it disabled UFW, and with it, my web server firewall ports. Luckily the fix for that was just a simple re-enable on the server:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"shell\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">sudo\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ufw\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> enable\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>(Note: UFW won’t show the VPN’s firewall rules, but iptables will.)\u003C/p>\n\u003Cp>Finally, for Vultr, they (wisely) set up an external firewall available for instances. This is generally a good idea, but trying to match it to the rules necessary for the VPN is a lot of work, so the final thing to do is to disable the firewall from the Vultr control panel. \u003Cstrong>Be sure your firewall is set up correctly on the server first.\u003C/strong>\u003C/p>",{"headings":415,"localImagePaths":430,"remoteImagePaths":431,"frontmatter":432,"imagePaths":435},[416,418,421,424,427],{"depth":43,"slug":417,"text":407},"algo",{"depth":43,"slug":419,"text":420},"problems-i-encountered","Problems I Encountered",{"depth":47,"slug":422,"text":423},"root-login-is-disabled","Root login is disabled",{"depth":47,"slug":425,"text":426},"ssh-connection-errors","SSH connection errors",{"depth":47,"slug":428,"text":429},"firewall-issues","Firewall issues",[],[],{"title":402,"pubDate":433,"tags":434,"description":403},["Date","2017-03-28T00:00:00.000Z"],[406,407,408],[]]